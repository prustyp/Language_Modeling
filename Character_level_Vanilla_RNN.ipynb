{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import operator\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "plt.style.use('seaborn-white')\n",
    "import re\n",
    "from nltk import tokenize\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load 3 Text Books and Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text files concatenation \n",
    "filenames = ['A_Journey_to_the_Center_of_the_Earth.txt', 'From_the_Earth_to_the_Moon.txt', 'The_master_of_the_world.txt']\n",
    "with open('/Users/pprusty05/google_drive/Deep_Learning/assignment3/Text_file_for_training.txt', 'w+') as outfile:\n",
    "    for fname in filenames:\n",
    "        with open(fname) as infile:\n",
    "            for line in infile:\n",
    "                outfile.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_token = \"*\"\n",
    "word_start_token = \"S\"\n",
    "word_end_token = \"E\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Text file, tokenize words and append word start and end token \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 273460 words.\n"
     ]
    }
   ],
   "source": [
    "# here I am tokenize the texts in to words and then appending S and E to word start and end\n",
    "with open ('Text_file_for_training.txt', 'r') as f:\n",
    "    \n",
    "    text = f.read()\n",
    "    tokenized_sent = nltk.word_tokenize(text)\n",
    "    words=[x.replace('\\n',' ') for x in tokenized_sent]\n",
    "    #words=[x.replace(' ','') for x in tokenized_sent]\n",
    "    \n",
    "    words = [re.sub('[^a-zA-Z0-9]', '', x)for x in words]\n",
    "    words = [word.lower() for word in words]\n",
    "    \n",
    "    new_words = [\"%s %s %s\" % (word_start_token, x, word_end_token) for x in words]\n",
    "    \n",
    "print( \"Parsed %d words.\" % (len(words)))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['project',\n",
       " 'gutenberg',\n",
       " 's',\n",
       " 'a',\n",
       " 'journey',\n",
       " 'to',\n",
       " 'the',\n",
       " 'centre',\n",
       " 'of',\n",
       " 'the',\n",
       " 'earth',\n",
       " '',\n",
       " 'by',\n",
       " 'jules',\n",
       " 'verne',\n",
       " 'this',\n",
       " 'ebook',\n",
       " 'is',\n",
       " 'for',\n",
       " 'the',\n",
       " 'use',\n",
       " 'of',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'at',\n",
       " 'no',\n",
       " 'cost',\n",
       " 'and',\n",
       " 'with',\n",
       " 'almost',\n",
       " 'no',\n",
       " 'restrictions',\n",
       " 'whatsoever',\n",
       " '',\n",
       " 'you',\n",
       " 'may',\n",
       " 'copy',\n",
       " 'it',\n",
       " '',\n",
       " 'give',\n",
       " 'it',\n",
       " 'away',\n",
       " 'or',\n",
       " 'reuse',\n",
       " 'it',\n",
       " 'under',\n",
       " 'the',\n",
       " 'terms',\n",
       " 'of',\n",
       " 'the',\n",
       " 'project',\n",
       " 'gutenberg',\n",
       " 'license',\n",
       " 'included',\n",
       " 'with',\n",
       " 'this',\n",
       " 'ebook',\n",
       " 'or',\n",
       " 'online',\n",
       " 'at',\n",
       " 'wwwgutenbergorg',\n",
       " 'title',\n",
       " '',\n",
       " 'a',\n",
       " 'journey',\n",
       " 'to',\n",
       " 'the',\n",
       " 'centre',\n",
       " 'of',\n",
       " 'the',\n",
       " 'earth',\n",
       " 'author',\n",
       " '',\n",
       " 'jules',\n",
       " 'verne',\n",
       " 'release',\n",
       " 'date',\n",
       " '',\n",
       " 'july',\n",
       " '18',\n",
       " '',\n",
       " '2006',\n",
       " '',\n",
       " 'ebook',\n",
       " '',\n",
       " '18857',\n",
       " '',\n",
       " 'last',\n",
       " 'updated',\n",
       " '',\n",
       " 'december',\n",
       " '27',\n",
       " '',\n",
       " '2012',\n",
       " 'language',\n",
       " '',\n",
       " 'english',\n",
       " 'character',\n",
       " 'set',\n",
       " 'encoding',\n",
       " '',\n",
       " 'ascii',\n",
       " '',\n",
       " 'start',\n",
       " 'of',\n",
       " 'this',\n",
       " 'project',\n",
       " 'gutenberg',\n",
       " 'ebook',\n",
       " 'centre',\n",
       " 'of',\n",
       " 'the',\n",
       " 'earth',\n",
       " '',\n",
       " 'produced',\n",
       " 'by',\n",
       " 'norm',\n",
       " 'wolcott',\n",
       " 'a',\n",
       " 'journey',\n",
       " 'to',\n",
       " 'the',\n",
       " 'centre',\n",
       " 'of',\n",
       " 'the',\n",
       " 'earth',\n",
       " 'by',\n",
       " 'jules',\n",
       " 'verne',\n",
       " '',\n",
       " 'redactor',\n",
       " 's',\n",
       " 'note',\n",
       " '',\n",
       " 'journey',\n",
       " 'to',\n",
       " 'the',\n",
       " 'centre',\n",
       " 'of',\n",
       " 'the',\n",
       " 'earth',\n",
       " 'is',\n",
       " 'number',\n",
       " 'v002',\n",
       " 'in',\n",
       " 'the',\n",
       " 'taves',\n",
       " 'and',\n",
       " 'michaluk',\n",
       " 'numbering',\n",
       " 'of',\n",
       " 'the',\n",
       " 'works',\n",
       " 'of',\n",
       " 'jules',\n",
       " 'verne',\n",
       " '',\n",
       " 'first',\n",
       " 'published',\n",
       " 'in',\n",
       " 'england',\n",
       " 'by',\n",
       " 'griffith',\n",
       " 'and',\n",
       " 'farran',\n",
       " '',\n",
       " '1871',\n",
       " '',\n",
       " 'this',\n",
       " 'edition',\n",
       " 'is',\n",
       " 'not',\n",
       " 'a',\n",
       " 'translation',\n",
       " 'at',\n",
       " 'all',\n",
       " 'but',\n",
       " 'a',\n",
       " 'complete',\n",
       " 'rewrite',\n",
       " 'of',\n",
       " 'the',\n",
       " 'novel',\n",
       " '',\n",
       " 'with',\n",
       " 'portions',\n",
       " 'added',\n",
       " 'and',\n",
       " 'omitted',\n",
       " '',\n",
       " 'and',\n",
       " 'names',\n",
       " 'changed',\n",
       " '',\n",
       " 'the',\n",
       " 'most',\n",
       " 'reprinted',\n",
       " 'version',\n",
       " '',\n",
       " 'it',\n",
       " 'is',\n",
       " 'entered',\n",
       " 'into',\n",
       " 'project',\n",
       " 'gutenberg',\n",
       " 'for',\n",
       " 'reference',\n",
       " 'purposes',\n",
       " 'only',\n",
       " '',\n",
       " 'a',\n",
       " 'better',\n",
       " 'translation',\n",
       " 'is',\n",
       " 'a',\n",
       " 'journey',\n",
       " 'into',\n",
       " 'the',\n",
       " 'interior',\n",
       " 'of',\n",
       " 'the',\n",
       " 'earth',\n",
       " 'translated',\n",
       " 'by',\n",
       " 'rev',\n",
       " '',\n",
       " 'f',\n",
       " 'a',\n",
       " 'malleson',\n",
       " '',\n",
       " 'also',\n",
       " 'available',\n",
       " 'on',\n",
       " 'project',\n",
       " 'gutenberg',\n",
       " '',\n",
       " '',\n",
       " 'table',\n",
       " 'of',\n",
       " 'contents',\n",
       " 'chapter',\n",
       " '1',\n",
       " 'my',\n",
       " 'uncle',\n",
       " 'makes',\n",
       " 'a',\n",
       " 'great',\n",
       " 'discovery',\n",
       " 'chapter',\n",
       " '2',\n",
       " 'the',\n",
       " 'mysterious',\n",
       " 'parchment',\n",
       " 'chapter',\n",
       " '3',\n",
       " 'an',\n",
       " 'astounding',\n",
       " 'discovery',\n",
       " 'chapter',\n",
       " '4',\n",
       " 'we',\n",
       " 'start',\n",
       " 'on',\n",
       " 'the',\n",
       " 'journey',\n",
       " 'chapter',\n",
       " '5',\n",
       " 'first',\n",
       " 'lessons',\n",
       " 'in',\n",
       " 'climbing',\n",
       " 'chapter',\n",
       " '6',\n",
       " 'our',\n",
       " 'voyage',\n",
       " 'to',\n",
       " 'iceland',\n",
       " 'chapter',\n",
       " '7',\n",
       " 'conversation',\n",
       " 'and',\n",
       " 'discovery',\n",
       " 'chapter',\n",
       " '8',\n",
       " 'the',\n",
       " 'eiderdown',\n",
       " 'hunter',\n",
       " '',\n",
       " 'off',\n",
       " 'at',\n",
       " 'last',\n",
       " 'chapter',\n",
       " '9',\n",
       " 'our',\n",
       " 'start',\n",
       " '',\n",
       " 'we',\n",
       " 'meet',\n",
       " 'with',\n",
       " 'adventures',\n",
       " 'by',\n",
       " 'the',\n",
       " 'way',\n",
       " 'chapter',\n",
       " '10',\n",
       " 'traveling',\n",
       " 'in',\n",
       " 'iceland',\n",
       " 'chapter',\n",
       " '11',\n",
       " 'we',\n",
       " 'reach',\n",
       " 'mount',\n",
       " 'sneffels',\n",
       " '',\n",
       " 'the',\n",
       " '',\n",
       " 'reykir',\n",
       " '',\n",
       " 'chapter',\n",
       " '12',\n",
       " 'the',\n",
       " 'ascent',\n",
       " 'of',\n",
       " 'mount',\n",
       " 'sneffels',\n",
       " 'chapter',\n",
       " '13',\n",
       " 'the',\n",
       " 'shadow',\n",
       " 'of',\n",
       " 'scartaris',\n",
       " 'chapter',\n",
       " '14',\n",
       " 'the',\n",
       " 'real',\n",
       " 'journey',\n",
       " 'commences',\n",
       " 'chapter',\n",
       " '15',\n",
       " 'we',\n",
       " 'continue',\n",
       " 'our',\n",
       " 'descent',\n",
       " 'chapter',\n",
       " '16',\n",
       " 'the',\n",
       " 'eastern',\n",
       " 'tunnel',\n",
       " 'chapter',\n",
       " '17',\n",
       " 'deeper',\n",
       " 'and',\n",
       " 'deeper',\n",
       " '',\n",
       " 'the',\n",
       " 'coal',\n",
       " 'mine',\n",
       " 'chapter',\n",
       " '18',\n",
       " 'the',\n",
       " 'wrong',\n",
       " 'road',\n",
       " '',\n",
       " 'chapter',\n",
       " '19',\n",
       " 'the',\n",
       " 'western',\n",
       " 'gallery',\n",
       " '',\n",
       " 'a',\n",
       " 'new',\n",
       " 'route',\n",
       " 'chapter',\n",
       " '20',\n",
       " 'water',\n",
       " '',\n",
       " 'where',\n",
       " 'is',\n",
       " 'it',\n",
       " '',\n",
       " 'a',\n",
       " 'bitter',\n",
       " 'disappointment',\n",
       " 'chapter',\n",
       " '21',\n",
       " 'under',\n",
       " 'the',\n",
       " 'ocean',\n",
       " 'chapter',\n",
       " '22',\n",
       " 'sunday',\n",
       " 'below',\n",
       " 'ground',\n",
       " 'chapter',\n",
       " '23',\n",
       " 'alone',\n",
       " 'chapter',\n",
       " '24',\n",
       " 'lost',\n",
       " '',\n",
       " 'chapter',\n",
       " '25',\n",
       " 'the',\n",
       " 'whispering',\n",
       " 'gallery',\n",
       " 'chapter',\n",
       " '26',\n",
       " 'a',\n",
       " 'rapid',\n",
       " 'recovery',\n",
       " 'chapter',\n",
       " '27',\n",
       " 'the',\n",
       " 'central',\n",
       " 'sea',\n",
       " 'chapter',\n",
       " '28',\n",
       " 'launching',\n",
       " 'the',\n",
       " 'raft',\n",
       " 'chapter',\n",
       " '29',\n",
       " 'on',\n",
       " 'the',\n",
       " 'waters',\n",
       " '',\n",
       " 'a',\n",
       " 'raft',\n",
       " 'voyage',\n",
       " 'chapter',\n",
       " '30',\n",
       " 'terrific',\n",
       " 'saurian',\n",
       " 'combat',\n",
       " 'chapter',\n",
       " '31',\n",
       " 'the',\n",
       " 'sea',\n",
       " 'monster',\n",
       " 'chapter',\n",
       " '32',\n",
       " 'the',\n",
       " 'battle',\n",
       " 'of',\n",
       " 'the',\n",
       " 'elements',\n",
       " 'chapter',\n",
       " '33',\n",
       " 'our',\n",
       " 'route',\n",
       " 'reversed',\n",
       " 'chapter',\n",
       " '34',\n",
       " 'a',\n",
       " 'voyage',\n",
       " 'of',\n",
       " 'discovery',\n",
       " 'chapter',\n",
       " '35',\n",
       " 'discovery',\n",
       " 'upon',\n",
       " 'discovery',\n",
       " 'chapter',\n",
       " '36',\n",
       " 'what',\n",
       " 'is',\n",
       " 'it',\n",
       " '',\n",
       " 'chapter',\n",
       " '37',\n",
       " 'the',\n",
       " 'mysterious',\n",
       " 'dagger',\n",
       " 'chapter',\n",
       " '38',\n",
       " 'no',\n",
       " 'outlet',\n",
       " '',\n",
       " 'blasting',\n",
       " 'the',\n",
       " 'rock',\n",
       " 'chapter',\n",
       " '39',\n",
       " 'the',\n",
       " 'explosion',\n",
       " 'and',\n",
       " 'its',\n",
       " 'results',\n",
       " 'chapter',\n",
       " '40',\n",
       " 'the',\n",
       " 'ape',\n",
       " 'gigans',\n",
       " 'chapter',\n",
       " '41',\n",
       " 'hunger',\n",
       " 'chapter',\n",
       " '42',\n",
       " 'the',\n",
       " 'volcanic',\n",
       " 'shaft',\n",
       " 'chapter',\n",
       " '43',\n",
       " 'daylight',\n",
       " 'at',\n",
       " 'last',\n",
       " 'chapter',\n",
       " '44',\n",
       " 'the',\n",
       " 'journey',\n",
       " 'ended',\n",
       " 'chapter',\n",
       " '1',\n",
       " 'my',\n",
       " 'uncle',\n",
       " 'makes',\n",
       " 'a',\n",
       " 'great',\n",
       " 'discovery',\n",
       " 'looking',\n",
       " 'back',\n",
       " 'to',\n",
       " 'all',\n",
       " 'that',\n",
       " 'has',\n",
       " 'occurred',\n",
       " 'to',\n",
       " 'me',\n",
       " 'since',\n",
       " 'that',\n",
       " 'eventful',\n",
       " 'day',\n",
       " '',\n",
       " 'i',\n",
       " 'am',\n",
       " 'scarcely',\n",
       " 'able',\n",
       " 'to',\n",
       " 'believe',\n",
       " 'in',\n",
       " 'the',\n",
       " 'reality',\n",
       " 'of',\n",
       " 'my',\n",
       " 'adventures',\n",
       " '',\n",
       " 'they',\n",
       " 'were',\n",
       " 'truly',\n",
       " 'so',\n",
       " 'wonderful',\n",
       " 'that',\n",
       " 'even',\n",
       " 'now',\n",
       " 'i',\n",
       " 'am',\n",
       " 'bewildered',\n",
       " 'when',\n",
       " 'i',\n",
       " 'think',\n",
       " 'of',\n",
       " 'them',\n",
       " '',\n",
       " 'my',\n",
       " 'uncle',\n",
       " 'was',\n",
       " 'a',\n",
       " 'german',\n",
       " '',\n",
       " 'having',\n",
       " 'married',\n",
       " 'my',\n",
       " 'mother',\n",
       " 's',\n",
       " 'sister',\n",
       " '',\n",
       " 'an',\n",
       " 'englishwoman',\n",
       " '',\n",
       " 'being',\n",
       " 'very',\n",
       " 'much',\n",
       " 'attached',\n",
       " 'to',\n",
       " 'his',\n",
       " 'fatherless',\n",
       " 'nephew',\n",
       " '',\n",
       " 'he',\n",
       " 'invited',\n",
       " 'me',\n",
       " 'to',\n",
       " 'study',\n",
       " 'under',\n",
       " 'him',\n",
       " 'in',\n",
       " 'his',\n",
       " 'home',\n",
       " 'in',\n",
       " 'the',\n",
       " 'fatherland',\n",
       " '',\n",
       " 'this',\n",
       " 'home',\n",
       " 'was',\n",
       " 'in',\n",
       " 'a',\n",
       " 'large',\n",
       " 'town',\n",
       " '',\n",
       " 'and',\n",
       " 'my',\n",
       " 'uncle',\n",
       " 'a',\n",
       " 'professor',\n",
       " 'of',\n",
       " 'philosophy',\n",
       " '',\n",
       " 'chemistry',\n",
       " '',\n",
       " 'geology',\n",
       " '',\n",
       " 'mineralogy',\n",
       " '',\n",
       " 'and',\n",
       " 'many',\n",
       " 'other',\n",
       " 'ologies',\n",
       " '',\n",
       " 'one',\n",
       " 'day',\n",
       " '',\n",
       " 'after',\n",
       " 'passing',\n",
       " 'some',\n",
       " 'hours',\n",
       " 'in',\n",
       " 'the',\n",
       " 'laboratory',\n",
       " '',\n",
       " 'my',\n",
       " 'uncle',\n",
       " 'being',\n",
       " 'absent',\n",
       " 'at',\n",
       " 'the',\n",
       " 'time',\n",
       " '',\n",
       " 'i',\n",
       " 'suddenly',\n",
       " 'felt',\n",
       " 'the',\n",
       " 'necessity',\n",
       " 'of',\n",
       " 'renovating',\n",
       " 'the',\n",
       " 'tissues',\n",
       " '',\n",
       " '',\n",
       " 'i',\n",
       " '',\n",
       " 'ie',\n",
       " '',\n",
       " 'i',\n",
       " '',\n",
       " '',\n",
       " 'i',\n",
       " 'was',\n",
       " 'hungry',\n",
       " '',\n",
       " 'and',\n",
       " 'was',\n",
       " 'about',\n",
       " 'to',\n",
       " 'rouse',\n",
       " 'up',\n",
       " 'our',\n",
       " 'old',\n",
       " 'french',\n",
       " 'cook',\n",
       " '',\n",
       " 'when',\n",
       " 'my',\n",
       " 'uncle',\n",
       " '',\n",
       " 'professor',\n",
       " 'von',\n",
       " 'hardwigg',\n",
       " '',\n",
       " 'suddenly',\n",
       " 'opened',\n",
       " 'the',\n",
       " 'street',\n",
       " 'door',\n",
       " '',\n",
       " 'and',\n",
       " 'came',\n",
       " 'rushing',\n",
       " 'upstairs',\n",
       " '',\n",
       " 'now',\n",
       " 'professor',\n",
       " 'hardwigg',\n",
       " '',\n",
       " 'my',\n",
       " 'worthy',\n",
       " 'uncle',\n",
       " '',\n",
       " 'is',\n",
       " 'by',\n",
       " 'no',\n",
       " 'means',\n",
       " 'a',\n",
       " 'bad',\n",
       " 'sort',\n",
       " 'of',\n",
       " 'man',\n",
       " '',\n",
       " 'he',\n",
       " 'is',\n",
       " '',\n",
       " 'however',\n",
       " '',\n",
       " 'choleric',\n",
       " 'and',\n",
       " 'original',\n",
       " '',\n",
       " 'to',\n",
       " 'bear',\n",
       " 'with',\n",
       " 'him',\n",
       " 'means',\n",
       " 'to',\n",
       " 'obey',\n",
       " '',\n",
       " 'and',\n",
       " 'scarcely',\n",
       " 'had',\n",
       " 'his',\n",
       " 'heavy',\n",
       " 'feet',\n",
       " 'resounded',\n",
       " 'within',\n",
       " 'our',\n",
       " 'joint',\n",
       " 'domicile',\n",
       " 'than',\n",
       " 'he',\n",
       " 'shouted',\n",
       " 'for',\n",
       " 'me',\n",
       " 'to',\n",
       " 'attend',\n",
       " 'upon',\n",
       " 'him',\n",
       " '',\n",
       " '',\n",
       " 'harry',\n",
       " '',\n",
       " 'harry',\n",
       " '',\n",
       " 'harry',\n",
       " '',\n",
       " '',\n",
       " 'i',\n",
       " 'hastened',\n",
       " 'to',\n",
       " 'obey',\n",
       " '',\n",
       " 'but',\n",
       " 'before',\n",
       " 'i',\n",
       " 'could',\n",
       " 'reach',\n",
       " 'his',\n",
       " 'room',\n",
       " '',\n",
       " 'jumping',\n",
       " 'three',\n",
       " 'steps',\n",
       " 'at',\n",
       " 'a',\n",
       " 'time',\n",
       " '',\n",
       " 'he',\n",
       " 'was',\n",
       " 'stamping',\n",
       " 'his',\n",
       " 'right',\n",
       " 'foot',\n",
       " 'upon',\n",
       " 'the',\n",
       " 'landing',\n",
       " '',\n",
       " '',\n",
       " 'harry',\n",
       " '',\n",
       " '',\n",
       " 'he',\n",
       " 'cried',\n",
       " '',\n",
       " 'in',\n",
       " 'a',\n",
       " 'frantic',\n",
       " 'tone',\n",
       " '',\n",
       " '',\n",
       " 'are',\n",
       " 'you',\n",
       " 'coming',\n",
       " 'up',\n",
       " '',\n",
       " '',\n",
       " 'now',\n",
       " 'to',\n",
       " 'tell',\n",
       " 'the',\n",
       " 'truth',\n",
       " '',\n",
       " 'at',\n",
       " 'that',\n",
       " 'moment',\n",
       " 'i',\n",
       " 'was',\n",
       " 'far',\n",
       " 'more',\n",
       " 'interested',\n",
       " 'in',\n",
       " 'the',\n",
       " 'question',\n",
       " 'as',\n",
       " 'to',\n",
       " 'what',\n",
       " 'was',\n",
       " 'to',\n",
       " 'constitute',\n",
       " 'our',\n",
       " 'dinner',\n",
       " 'than',\n",
       " 'in',\n",
       " 'any',\n",
       " 'problem',\n",
       " 'of',\n",
       " 'science',\n",
       " '',\n",
       " 'to',\n",
       " 'me',\n",
       " 'soup',\n",
       " 'was',\n",
       " 'more',\n",
       " 'interesting',\n",
       " 'than',\n",
       " 'soda',\n",
       " '',\n",
       " 'an',\n",
       " 'omelette',\n",
       " 'more',\n",
       " 'tempting',\n",
       " 'than',\n",
       " 'arithmetic',\n",
       " '',\n",
       " 'and',\n",
       " 'an',\n",
       " 'artichoke',\n",
       " 'of',\n",
       " 'ten',\n",
       " 'times',\n",
       " 'more',\n",
       " 'value',\n",
       " 'than',\n",
       " 'any',\n",
       " 'amount',\n",
       " 'of',\n",
       " 'asbestos',\n",
       " '',\n",
       " 'but',\n",
       " 'my',\n",
       " 'uncle',\n",
       " 'was',\n",
       " 'not',\n",
       " 'a',\n",
       " 'man',\n",
       " 'to',\n",
       " 'be',\n",
       " 'kept',\n",
       " 'waiting',\n",
       " '',\n",
       " 'so',\n",
       " 'adjourning',\n",
       " 'therefore',\n",
       " 'all',\n",
       " 'minor',\n",
       " 'questions',\n",
       " '',\n",
       " 'i',\n",
       " 'presented',\n",
       " 'myself',\n",
       " 'before',\n",
       " 'him',\n",
       " '',\n",
       " 'he',\n",
       " 'was',\n",
       " 'a',\n",
       " 'very',\n",
       " 'learned',\n",
       " 'man',\n",
       " '',\n",
       " 'now',\n",
       " 'most',\n",
       " 'persons',\n",
       " 'in',\n",
       " 'this',\n",
       " 'category',\n",
       " 'supply',\n",
       " 'themselves',\n",
       " 'with',\n",
       " 'information',\n",
       " '',\n",
       " 'as',\n",
       " 'peddlers',\n",
       " 'do',\n",
       " 'with',\n",
       " 'goods',\n",
       " '',\n",
       " 'for',\n",
       " 'the',\n",
       " 'benefit',\n",
       " 'of',\n",
       " 'others',\n",
       " '',\n",
       " 'and',\n",
       " 'lay',\n",
       " 'up',\n",
       " 'stores',\n",
       " 'in',\n",
       " 'order',\n",
       " 'to',\n",
       " 'diffuse',\n",
       " 'them',\n",
       " 'abroad',\n",
       " 'for',\n",
       " 'the',\n",
       " 'benefit',\n",
       " 'of',\n",
       " 'society',\n",
       " 'in',\n",
       " 'general',\n",
       " '',\n",
       " 'not',\n",
       " 'so',\n",
       " 'my',\n",
       " 'excellent',\n",
       " 'uncle',\n",
       " '',\n",
       " 'professor',\n",
       " 'hardwigg',\n",
       " '',\n",
       " 'he',\n",
       " 'studied',\n",
       " '',\n",
       " 'he',\n",
       " 'consumed',\n",
       " 'the',\n",
       " 'midnight',\n",
       " 'oil',\n",
       " '',\n",
       " 'he',\n",
       " 'pored',\n",
       " 'over',\n",
       " 'heavy',\n",
       " 'tomes',\n",
       " '',\n",
       " 'and',\n",
       " 'digested',\n",
       " 'huge',\n",
       " 'quartos',\n",
       " 'and',\n",
       " 'folios',\n",
       " 'in',\n",
       " 'order',\n",
       " 'to',\n",
       " 'keep',\n",
       " 'the',\n",
       " 'knowledge',\n",
       " 'acquired',\n",
       " 'to',\n",
       " 'himself',\n",
       " '',\n",
       " 'there',\n",
       " 'was',\n",
       " 'a',\n",
       " 'reason',\n",
       " '',\n",
       " 'and',\n",
       " 'it',\n",
       " 'may',\n",
       " 'be',\n",
       " 'regarded',\n",
       " 'as',\n",
       " ...]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1336569 characters, 93 unique\n"
     ]
    }
   ],
   "source": [
    "#here I am tokenizing the words to character and getting the uniques characters fromthe text file\n",
    "# Tokenize the words into char\n",
    "train_data = open('Text_file_for_training.txt', 'r').read() \n",
    "chars = list(set(train_data))\n",
    "data_size,char_size = len(train_data), len(chars)\n",
    "print(\"data has %d characters, %d unique\" %(data_size, char_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'e': 135934, 't': 99788, 'a': 81042, 'o': 79615, 'i': 72483, 'n': 70915, 's': 64262, 'r': 63853, 'h': 61065, 'd': 42435, 'l': 41951, 'c': 31566, 'u': 30740, 'm': 25989, 'f': 25020, 'w': 23665, 'p': 19989, 'g': 18503, 'y': 18486, 'b': 16484, 'v': 10389, 'k': 6356, 'x': 2200, 'j': 1757, 'q': 1128, '0': 775, 'z': 608, '1': 481, '2': 275, '3': 192, '4': 184, '5': 179, '8': 162, '6': 123, '7': 107, '9': 93})\n"
     ]
    }
   ],
   "source": [
    "# Count the character frequencies\n",
    "char_freq=(Counter(''.join(words)))\n",
    "print(char_freq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the sentences into words\n",
    "tokenized_char = [list(word) for word in new_words]\n",
    "print(tokenized_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all words not in our vocabulary with the unknown token\n",
    "for i, wrd in enumerate(tokenized_char):\n",
    "    tokenized_char[i] = [w if w in char_to_ix else unknown_token for w in wrd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example sentence: 'S project E'\n",
      "\n",
      "Example sentence after Pre-processing: '['S', ' ', 'p', 'r', 'o', 'j', 'e', 'c', 't', ' ', 'E']'\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nExample sentence: '%s'\" % new_words[0])\n",
    "print(\"\\nExample sentence after Pre-processing: '%s'\" % tokenized_char[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XTrain and yTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training data\n",
    "XTrain = np.asarray([[char_to_ix[w] for w in sent[:-1]] for sent in tokenized_char])\n",
    "yTrain = np.asarray([[char_to_ix[w] for w in sent[1:]] for sent in tokenized_char])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(273460,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XTrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "S   e a r t h  \n",
      "[5, 36, 64, 47, 39, 12, 10, 36]\n",
      "\n",
      "y:\n",
      "  e a r t h   E\n",
      "[36, 64, 47, 39, 12, 10, 36, 60]\n"
     ]
    }
   ],
   "source": [
    "# Print an training data example\n",
    "x_example, y_example = XTrain[10], yTrain[10]\n",
    "print (\"x:\\n%s\\n%s\" % (\" \".join([ix_to_char[x] for x in x_example]), x_example))\n",
    "print (\"\\ny:\\n%s\\n%s\" % (\" \".join([ix_to_char[x] for x in y_example]), y_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Character level Vanilla RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNVanilla:\n",
    "     \n",
    "    def __init__(self, word_dim, hidden_dim=100, bptt_truncate=10):\n",
    "        \n",
    "        # Assign instance variables\n",
    "        self.word_dim = word_dim   #size of the vocabulary\n",
    "        self.hidden_dim = hidden_dim  # size of hidden layer\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        \n",
    "        # Randomly initialize the network parameters\n",
    "        self.U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))\n",
    "        \n",
    "        \n",
    "    def softmax(self,x):\n",
    "        xt = np.exp(x - np.max(x))\n",
    "        return xt / np.sum(xt)\n",
    "    \n",
    "    def forward_propagation(self, x):\n",
    "        # The total number of time steps\n",
    "        T = len(x)\n",
    "\n",
    "        # During forward propagation we save all hidden states in s because need them later.\n",
    "\n",
    "        # We add one additional element for the initial hidden, which we set to 0\n",
    "        s = np.zeros((T + 1, self.hidden_dim))\n",
    "        s[-1] = np.zeros(self.hidden_dim)\n",
    "\n",
    "        # The outputs at each time step. Again, we save them for later.\n",
    "        o = np.zeros((T, self.word_dim))\n",
    "\n",
    "        # For each time step...\n",
    "        for t in np.arange(T):\n",
    "            # Note that we are indxing U by x[t]. This is the same as multiplying U with a one-hot vector.\n",
    "            s[t] = np.tanh(self.U[:,x[t]] + self.W.dot(s[t-1]))\n",
    "            o[t] = self.softmax(self.V.dot(s[t]))\n",
    "        return [o, s] \n",
    "\n",
    "    def predict(self, x):\n",
    "        # Perform forward propagation and return index of the highest score\n",
    "        o, s = self.forward_propagation(x)\n",
    "        return np.argmax(o, axis=1)\n",
    "\n",
    "    def calculate_total_loss(self, x, y):\n",
    "        L = 0\n",
    "\n",
    "        # For each sentence...\n",
    "        for i in np.arange(len(y)):\n",
    "            o, s = self.forward_propagation(x[i])\n",
    "\n",
    "            # We only care about our prediction of the \"correct\" words\n",
    "            correct_word_predictions = o[np.arange(len(y[i])), y[i]]\n",
    "\n",
    "            # Add to the loss based on how off we were\n",
    "            L += -1 * sum(np.log(correct_word_predictions))\n",
    "        return L\n",
    " \n",
    "    def calculate_loss(self, x, y):\n",
    "        # Divide the total loss by the number of training examples\n",
    "        N = sum((len(y_i) for y_i in y))\n",
    "        return self.calculate_total_loss(x,y)/N\n",
    "    def bptt(self, x, y):\n",
    "        T = len(y)\n",
    "        # Perform forward propagation\n",
    "        o, s = self.forward_propagation(x)\n",
    "        # We accumulate the gradients in these variables\n",
    "        dLdU = np.zeros(self.U.shape)\n",
    "        dLdV = np.zeros(self.V.shape)\n",
    "        dLdW = np.zeros(self.W.shape)\n",
    "        delta_o = o\n",
    "        delta_o[np.arange(len(y)), y] -= 1.\n",
    "        # For each output backwards...\n",
    "        for t in np.arange(T)[::-1]:\n",
    "            dLdV += np.outer(delta_o[t], s[t].T)\n",
    "\n",
    "            # Initial delta calculation\n",
    "            delta_t = self.V.T.dot(delta_o[t]) * (1 - (s[t] ** 2))\n",
    "\n",
    "            # Backpropagation through time (for at most self.bptt_truncate steps)\n",
    "            for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]:\n",
    "\n",
    "                # print \"Backpropagation step t=%d bptt step=%d \" % (t, bptt_step)\n",
    "                dLdW += np.outer(delta_t, s[bptt_step-1])              \n",
    "                dLdU[:,x[bptt_step]] += delta_t\n",
    "\n",
    "                # Update delta for next step\n",
    "                delta_t = self.W.T.dot(delta_t) * (1 - s[bptt_step-1] ** 2)\n",
    "        return [dLdU, dLdV, dLdW]\n",
    "    \n",
    "    def gradient_check(self, x, y, h=0.001, error_threshold=0.01):\n",
    "        # Calculate the gradients using backpropagation. We want to checker if these are correct.\n",
    "        bptt_gradients = self.bptt(x, y)\n",
    "\n",
    "        # List of all parameters we want to check.\n",
    "        model_parameters = ['U', 'V', 'W']\n",
    "\n",
    "        # Gradient check for each parameter\n",
    "        for pidx, pname in enumerate(model_parameters):\n",
    "            # Get the actual parameter value from the mode, e.g. model.W\n",
    "            parameter = operator.attrgetter(pname)(self)\n",
    "            print (\"Performing gradient check for parameter %s with size %d.\" % (pname, np.prod(parameter.shape)))\n",
    "            # Iterate over each element of the parameter matrix, e.g. (0,0), (0,1), ...\n",
    "            it = np.nditer(parameter, flags=['multi_index'], op_flags=['readwrite'])\n",
    "            while not it.finished:\n",
    "                ix = it.multi_index\n",
    "                # Save the original value so we can reset it later\n",
    "                original_value = parameter[ix]\n",
    "                # Estimate the gradient using (f(x+h) - f(x-h))/(2*h)\n",
    "                parameter[ix] = original_value + h\n",
    "                gradplus = self.calculate_total_loss([x],[y])\n",
    "                parameter[ix] = original_value - h\n",
    "                gradminus = self.calculate_total_loss([x],[y])\n",
    "                estimated_gradient = (gradplus - gradminus)/(2*h)\n",
    "                # Reset parameter to original value\n",
    "                parameter[ix] = original_value\n",
    "                # The gradient for this parameter calculated using backpropagation\n",
    "                backprop_gradient = bptt_gradients[pidx][ix]\n",
    "                # calculate The relative error: (|x - y|/(|x| + |y|))\n",
    "                relative_error = np.abs(backprop_gradient - estimated_gradient)/(np.abs(backprop_gradient) + np.abs(estimated_gradient))\n",
    "                # If the error is to large fail the gradient check\n",
    "                if relative_error > error_threshold:\n",
    "                    print (\"Gradient Check ERROR: parameter=%s ix=%s\" % (pname, ix))\n",
    "                    print (\"+h Loss: %f\" % gradplus)\n",
    "                    print (\"-h Loss: %f\" % gradminus)\n",
    "                    print (\"Estimated_gradient: %f\" % estimated_gradient)\n",
    "                    print (\"Backpropagation gradient: %f\" % backprop_gradient)\n",
    "                    print (\"Relative Error: %f\" % relative_error)\n",
    "                    return\n",
    "                it.iternext()\n",
    "            print (\"Gradient check for parameter %s passed.\" % (pname))\n",
    "            \n",
    "            # Performs one step of SGD.\n",
    "    def numpy_sdg_step(self, x, y, learning_rate):\n",
    "        # Calculate the gradients\n",
    "        dLdU, dLdV, dLdW = self.bptt(x, y)\n",
    "        # Change parameters according to gradients and learning rate\n",
    "        self.U -= learning_rate * dLdU\n",
    "        self.V -= learning_rate * dLdV\n",
    "        self.W -= learning_rate * dLdW\n",
    "        \n",
    "    def generate_sentence(self,model):\n",
    "       \n",
    "    # We start the sentence with the start token\n",
    "        new_sentence = [(XTrain[10])[0]]\n",
    "\n",
    "        # Repeat until we get an end token\n",
    "        while not new_sentence[-1] == char_to_ix[word_end_token]:\n",
    "            next_word_probs,_ = model.forward_propagation(new_sentence)\n",
    "            sampled_word = char_to_ix[unknown_token]\n",
    "            # We don't want to sample unknown words\n",
    "            while sampled_word == char_to_ix[unknown_token]:\n",
    "                samples = np.random.multinomial(1, next_word_probs[-1])\n",
    "                sampled_word = np.argmax(samples)\n",
    "            new_sentence.append(sampled_word)\n",
    "        sentence_str = [ix_to_char[x] for x in new_sentence[1:-1]]\n",
    "        return sentence_str\n",
    "\n",
    "\n",
    "    def train_with_sgd(self, X_train, y_train, break_points_list, learning_rate=0.005, nepoch=100, evaluate_loss_after=5):\n",
    "        # We keep track of the losses so we can plot them later\n",
    "        losses = []\n",
    "        loss_per_epoch=[]\n",
    "        num_examples_seen = 0\n",
    "        for epoch in range(nepoch):\n",
    "            # Optionally evaluate the loss\n",
    "            if (epoch % evaluate_loss_after == 0):\n",
    "                loss = model.calculate_loss(X_train, y_train)\n",
    "                losses.append((num_examples_seen, loss))\n",
    "                loss_per_epoch.append(loss)\n",
    "                time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                print (\"%s: Loss after num_examples_seen=%d epoch=%d: %f\" % (time, num_examples_seen, epoch, loss))\n",
    "                # Adjust the learning rate if loss increases\n",
    "                if (len(losses) > 1 and losses[-1][1] > losses[-2][1]):\n",
    "                    learning_rate = learning_rate * 0.5 \n",
    "                    print (\"Setting learning rate to %f\" % learning_rate)\n",
    "                sys.stdout.flush()\n",
    "            if epoch in break_points_list:\n",
    "                print(\"========================\")\n",
    "                print(\"Start Pred -- \")\n",
    "                predictions = model.predict(XTrain[10])\n",
    "                print(\"input_characters>\")\n",
    "                print(XTrain[10])\n",
    "                print('%s'%\" \".join([ix_to_char[x] for x in XTrain[10]]))\n",
    "                #print(predictions.shape)\n",
    "                print(\"output_characters>\")\n",
    "                print(predictions)\n",
    "                print('%s'%\" \".join([ix_to_char[x] for x in predictions]))\n",
    "                \n",
    "                num_sentences = 1\n",
    "                senten_min_length = 5\n",
    "\n",
    "                for i in range(num_sentences):\n",
    "                    sent = []\n",
    "                    # We want long sentences, not sentences with one or two words\n",
    "                    while len(sent) < senten_min_length:\n",
    "                        sent = self.generate_sentence(model)\n",
    "                    print(\"Generate Words by feeding the first character to the model at epoch--\",epoch)\n",
    "                    print (\" \".join(sent))\n",
    "                \n",
    "                print(\"========================\")\n",
    "\n",
    "\n",
    "            # For each training example...\n",
    "            for i in range(len(y_train)):\n",
    "                # One SGD step\n",
    "                model.numpy_sdg_step(X_train[i], y_train[i], learning_rate)\n",
    "                num_examples_seen += 1\n",
    "            #Plot the Loss Curves\n",
    "        print(loss_per_epoch)\n",
    "        plt.figure(figsize=[8,6])\n",
    "        plt.plot(loss_per_epoch,'r',linewidth=3.0)\n",
    "        plt.legend(['Training loss'],fontsize=18)\n",
    "        plt.xlabel('Epochs ',fontsize=16)\n",
    "        plt.ylabel('Loss',fontsize=16)\n",
    "        plt.title('Loss Curves',fontsize=16)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report gradient check routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing gradient check for parameter U with size 1000.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pprusty05/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:120: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check for parameter U passed.\n",
      "Performing gradient check for parameter V with size 1000.\n",
      "Gradient check for parameter V passed.\n",
      "Performing gradient check for parameter W with size 100.\n",
      "Gradient check for parameter W passed.\n"
     ]
    }
   ],
   "source": [
    "# To avoid performing millions of expensive calculations we use a smaller vocabulary size for checking.\n",
    "grad_check_vocab_size = 100\n",
    "np.random.seed(10)\n",
    "model = RNNVanilla(grad_check_vocab_size, 10, bptt_truncate=1000)\n",
    "model.gradient_check([0,1,2,3], [1,2,3,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report the training loss vs epochs as a plot, output of the network at breakpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-06 23:17:34: Loss after num_examples_seen=0 epoch=0: 4.520456\n",
      "2019-11-06 23:17:36: Loss after num_examples_seen=500 epoch=1: 2.232250\n",
      "========================\n",
      "Start Pred -- \n",
      "input_characters>\n",
      "[5, 36, 64, 47, 39, 12, 10, 36]\n",
      "S   e a r t h  \n",
      "output_characters>\n",
      "[36 60 10 36 36 36 36 60]\n",
      "  E h         E\n",
      "Generate Words by feeding the first character to the model at epoch-- 1\n",
      "  2 < l r e  \n",
      "========================\n",
      "2019-11-06 23:17:37: Loss after num_examples_seen=1000 epoch=2: 1.994184\n",
      "2019-11-06 23:17:38: Loss after num_examples_seen=1500 epoch=3: 1.874169\n",
      "2019-11-06 23:17:39: Loss after num_examples_seen=2000 epoch=4: 1.779009\n",
      "2019-11-06 23:17:40: Loss after num_examples_seen=2500 epoch=5: 1.714371\n",
      "2019-11-06 23:17:41: Loss after num_examples_seen=3000 epoch=6: 1.665611\n",
      "2019-11-06 23:17:43: Loss after num_examples_seen=3500 epoch=7: 1.624559\n",
      "2019-11-06 23:17:44: Loss after num_examples_seen=4000 epoch=8: 1.588366\n",
      "2019-11-06 23:17:45: Loss after num_examples_seen=4500 epoch=9: 1.555591\n",
      "2019-11-06 23:17:46: Loss after num_examples_seen=5000 epoch=10: 1.525418\n",
      "========================\n",
      "Start Pred -- \n",
      "input_characters>\n",
      "[5, 36, 64, 47, 39, 12, 10, 36]\n",
      "S   e a r t h  \n",
      "output_characters>\n",
      "[36  6 39 36 12 64 64 60]\n",
      "  c r   t e e E\n",
      "Generate Words by feeding the first character to the model at epoch-- 10\n",
      "  v P a s c P n e s t i t  \n",
      "========================\n",
      "2019-11-06 23:17:48: Loss after num_examples_seen=5500 epoch=11: 1.497564\n",
      "2019-11-06 23:17:49: Loss after num_examples_seen=6000 epoch=12: 1.471862\n",
      "2019-11-06 23:17:51: Loss after num_examples_seen=6500 epoch=13: 1.448234\n",
      "2019-11-06 23:17:52: Loss after num_examples_seen=7000 epoch=14: 1.426906\n",
      "2019-11-06 23:17:54: Loss after num_examples_seen=7500 epoch=15: 1.408080\n",
      "2019-11-06 23:17:55: Loss after num_examples_seen=8000 epoch=16: 1.391450\n",
      "2019-11-06 23:17:56: Loss after num_examples_seen=8500 epoch=17: 1.376226\n",
      "2019-11-06 23:17:58: Loss after num_examples_seen=9000 epoch=18: 1.361465\n",
      "2019-11-06 23:17:59: Loss after num_examples_seen=9500 epoch=19: 1.346176\n",
      "2019-11-06 23:18:01: Loss after num_examples_seen=10000 epoch=20: 1.329917\n",
      "========================\n",
      "Start Pred -- \n",
      "input_characters>\n",
      "[5, 36, 64, 47, 39, 12, 10, 36]\n",
      "S   e a r t h  \n",
      "output_characters>\n",
      "[36  6 83 36 73 10 36 60]\n",
      "  c n   l h   E\n",
      "Generate Words by feeding the first character to the model at epoch-- 20\n",
      "  t h e  \n",
      "========================\n",
      "2019-11-06 23:18:05: Loss after num_examples_seen=10500 epoch=21: 1.313007\n",
      "2019-11-06 23:18:07: Loss after num_examples_seen=11000 epoch=22: 1.296169\n",
      "2019-11-06 23:18:08: Loss after num_examples_seen=11500 epoch=23: 1.280451\n",
      "2019-11-06 23:18:09: Loss after num_examples_seen=12000 epoch=24: 1.266373\n",
      "2019-11-06 23:18:10: Loss after num_examples_seen=12500 epoch=25: 1.253287\n",
      "2019-11-06 23:18:12: Loss after num_examples_seen=13000 epoch=26: 1.239822\n",
      "2019-11-06 23:18:13: Loss after num_examples_seen=13500 epoch=27: 1.225519\n",
      "2019-11-06 23:18:14: Loss after num_examples_seen=14000 epoch=28: 1.211717\n",
      "2019-11-06 23:18:17: Loss after num_examples_seen=14500 epoch=29: 1.199441\n",
      "2019-11-06 23:18:18: Loss after num_examples_seen=15000 epoch=30: 1.189273\n",
      "========================\n",
      "Start Pred -- \n",
      "input_characters>\n",
      "[5, 36, 64, 47, 39, 12, 10, 36]\n",
      "S   e a r t h  \n",
      "output_characters>\n",
      "[36  6 83 69 73 10 36 60]\n",
      "  c n s l h   E\n",
      "Generate Words by feeding the first character to the model at epoch-- 30\n",
      "  i n d  \n",
      "========================\n",
      "2019-11-06 23:18:19: Loss after num_examples_seen=15500 epoch=31: 1.181536\n",
      "2019-11-06 23:18:20: Loss after num_examples_seen=16000 epoch=32: 1.175508\n",
      "2019-11-06 23:18:22: Loss after num_examples_seen=16500 epoch=33: 1.170920\n",
      "2019-11-06 23:18:23: Loss after num_examples_seen=17000 epoch=34: 1.168130\n",
      "2019-11-06 23:18:24: Loss after num_examples_seen=17500 epoch=35: 1.168184\n",
      "Setting learning rate to 0.002500\n",
      "2019-11-06 23:18:25: Loss after num_examples_seen=18000 epoch=36: 1.109476\n",
      "2019-11-06 23:18:28: Loss after num_examples_seen=18500 epoch=37: 1.099595\n",
      "2019-11-06 23:18:29: Loss after num_examples_seen=19000 epoch=38: 1.094097\n",
      "2019-11-06 23:18:30: Loss after num_examples_seen=19500 epoch=39: 1.090542\n",
      "2019-11-06 23:18:32: Loss after num_examples_seen=20000 epoch=40: 1.086656\n",
      "========================\n",
      "Start Pred -- \n",
      "input_characters>\n",
      "[5, 36, 64, 47, 39, 12, 10, 36]\n",
      "S   e a r t h  \n",
      "output_characters>\n",
      "[36  6 83 69 12 10 36 60]\n",
      "  c n s t h   E\n",
      "Generate Words by feeding the first character to the model at epoch-- 40\n",
      "  t h e  \n",
      "========================\n",
      "2019-11-06 23:18:33: Loss after num_examples_seen=20500 epoch=41: 1.084192\n",
      "2019-11-06 23:18:34: Loss after num_examples_seen=21000 epoch=42: 1.081211\n",
      "2019-11-06 23:18:36: Loss after num_examples_seen=21500 epoch=43: 1.079227\n",
      "2019-11-06 23:18:37: Loss after num_examples_seen=22000 epoch=44: 1.076696\n",
      "2019-11-06 23:18:38: Loss after num_examples_seen=22500 epoch=45: 1.074860\n",
      "2019-11-06 23:18:39: Loss after num_examples_seen=23000 epoch=46: 1.072520\n",
      "2019-11-06 23:18:40: Loss after num_examples_seen=23500 epoch=47: 1.070601\n",
      "2019-11-06 23:18:42: Loss after num_examples_seen=24000 epoch=48: 1.068343\n",
      "2019-11-06 23:18:43: Loss after num_examples_seen=24500 epoch=49: 1.066351\n",
      "2019-11-06 23:18:44: Loss after num_examples_seen=25000 epoch=50: 1.064209\n",
      "========================\n",
      "Start Pred -- \n",
      "input_characters>\n",
      "[5, 36, 64, 47, 39, 12, 10, 36]\n",
      "S   e a r t h  \n",
      "output_characters>\n",
      "[36  6 47 69 12 10 36 60]\n",
      "  c a s t h   E\n",
      "Generate Words by feeding the first character to the model at epoch-- 50\n",
      "  c h a p t e r  \n",
      "========================\n",
      "2019-11-06 23:18:46: Loss after num_examples_seen=25500 epoch=51: 1.062246\n",
      "2019-11-06 23:18:47: Loss after num_examples_seen=26000 epoch=52: 1.060250\n",
      "2019-11-06 23:18:49: Loss after num_examples_seen=26500 epoch=53: 1.058366\n",
      "2019-11-06 23:18:50: Loss after num_examples_seen=27000 epoch=54: 1.056493\n",
      "2019-11-06 23:18:51: Loss after num_examples_seen=27500 epoch=55: 1.054681\n",
      "2019-11-06 23:18:53: Loss after num_examples_seen=28000 epoch=56: 1.052883\n",
      "2019-11-06 23:18:54: Loss after num_examples_seen=28500 epoch=57: 1.051111\n",
      "2019-11-06 23:18:55: Loss after num_examples_seen=29000 epoch=58: 1.049345\n",
      "2019-11-06 23:18:56: Loss after num_examples_seen=29500 epoch=59: 1.047584\n",
      "2019-11-06 23:18:57: Loss after num_examples_seen=30000 epoch=60: 1.045821\n",
      "========================\n",
      "Start Pred -- \n",
      "input_characters>\n",
      "[5, 36, 64, 47, 39, 12, 10, 36]\n",
      "S   e a r t h  \n",
      "output_characters>\n",
      "[36  6 47 69 12 10 36 60]\n",
      "  c a s t h   E\n",
      "Generate Words by feeding the first character to the model at epoch-- 60\n",
      "  t h e  \n",
      "========================\n",
      "2019-11-06 23:18:58: Loss after num_examples_seen=30500 epoch=61: 1.044056\n",
      "2019-11-06 23:19:00: Loss after num_examples_seen=31000 epoch=62: 1.042288\n",
      "2019-11-06 23:19:01: Loss after num_examples_seen=31500 epoch=63: 1.040520\n",
      "2019-11-06 23:19:02: Loss after num_examples_seen=32000 epoch=64: 1.038754\n",
      "2019-11-06 23:19:04: Loss after num_examples_seen=32500 epoch=65: 1.036996\n",
      "2019-11-06 23:19:05: Loss after num_examples_seen=33000 epoch=66: 1.035251\n",
      "2019-11-06 23:19:06: Loss after num_examples_seen=33500 epoch=67: 1.033524\n",
      "2019-11-06 23:19:07: Loss after num_examples_seen=34000 epoch=68: 1.031821\n",
      "2019-11-06 23:19:08: Loss after num_examples_seen=34500 epoch=69: 1.030147\n",
      "2019-11-06 23:19:10: Loss after num_examples_seen=35000 epoch=70: 1.028504\n",
      "========================\n",
      "Start Pred -- \n",
      "input_characters>\n",
      "[5, 36, 64, 47, 39, 12, 10, 36]\n",
      "S   e a r t h  \n",
      "output_characters>\n",
      "[36  6 47 39 12 10 36 60]\n",
      "  c a r t h   E\n",
      "Generate Words by feeding the first character to the model at epoch-- 70\n",
      "  w h a t  \n",
      "========================\n",
      "2019-11-06 23:19:11: Loss after num_examples_seen=35500 epoch=71: 1.026891\n",
      "2019-11-06 23:19:12: Loss after num_examples_seen=36000 epoch=72: 1.025307\n",
      "2019-11-06 23:19:13: Loss after num_examples_seen=36500 epoch=73: 1.023746\n",
      "2019-11-06 23:19:14: Loss after num_examples_seen=37000 epoch=74: 1.022205\n",
      "2019-11-06 23:19:15: Loss after num_examples_seen=37500 epoch=75: 1.020684\n",
      "2019-11-06 23:19:17: Loss after num_examples_seen=38000 epoch=76: 1.019193\n",
      "2019-11-06 23:19:18: Loss after num_examples_seen=38500 epoch=77: 1.017736\n",
      "2019-11-06 23:19:19: Loss after num_examples_seen=39000 epoch=78: 1.016316\n",
      "2019-11-06 23:19:20: Loss after num_examples_seen=39500 epoch=79: 1.014935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-06 23:19:21: Loss after num_examples_seen=40000 epoch=80: 1.013592\n",
      "========================\n",
      "Start Pred -- \n",
      "input_characters>\n",
      "[5, 36, 64, 47, 39, 12, 10, 36]\n",
      "S   e a r t h  \n",
      "output_characters>\n",
      "[36  6 47 39 12 10 36 60]\n",
      "  c a r t h   E\n",
      "Generate Words by feeding the first character to the model at epoch-- 80\n",
      "  m h e r t i\n",
      "========================\n",
      "2019-11-06 23:19:23: Loss after num_examples_seen=40500 epoch=81: 1.012285\n",
      "2019-11-06 23:19:24: Loss after num_examples_seen=41000 epoch=82: 1.011018\n",
      "2019-11-06 23:19:25: Loss after num_examples_seen=41500 epoch=83: 1.009803\n",
      "2019-11-06 23:19:26: Loss after num_examples_seen=42000 epoch=84: 1.008655\n",
      "2019-11-06 23:19:27: Loss after num_examples_seen=42500 epoch=85: 1.007593\n",
      "2019-11-06 23:19:29: Loss after num_examples_seen=43000 epoch=86: 1.006641\n",
      "2019-11-06 23:19:30: Loss after num_examples_seen=43500 epoch=87: 1.005820\n",
      "2019-11-06 23:19:31: Loss after num_examples_seen=44000 epoch=88: 1.005151\n",
      "2019-11-06 23:19:32: Loss after num_examples_seen=44500 epoch=89: 1.004631\n",
      "2019-11-06 23:19:33: Loss after num_examples_seen=45000 epoch=90: 1.004225\n",
      "========================\n",
      "Start Pred -- \n",
      "input_characters>\n",
      "[5, 36, 64, 47, 39, 12, 10, 36]\n",
      "S   e a r t h  \n",
      "output_characters>\n",
      "[36  6 47 39 12 10 36 60]\n",
      "  c a r t h   E\n",
      "Generate Words by feeding the first character to the model at epoch-- 90\n",
      "  u n d e r  \n",
      "========================\n",
      "2019-11-06 23:19:34: Loss after num_examples_seen=45500 epoch=91: 1.003875\n",
      "2019-11-06 23:19:36: Loss after num_examples_seen=46000 epoch=92: 1.003543\n",
      "2019-11-06 23:19:37: Loss after num_examples_seen=46500 epoch=93: 1.003224\n",
      "2019-11-06 23:19:38: Loss after num_examples_seen=47000 epoch=94: 1.002931\n",
      "2019-11-06 23:19:39: Loss after num_examples_seen=47500 epoch=95: 1.002680\n",
      "2019-11-06 23:19:41: Loss after num_examples_seen=48000 epoch=96: 1.002482\n",
      "2019-11-06 23:19:42: Loss after num_examples_seen=48500 epoch=97: 1.002338\n",
      "2019-11-06 23:19:43: Loss after num_examples_seen=49000 epoch=98: 1.002245\n",
      "2019-11-06 23:19:44: Loss after num_examples_seen=49500 epoch=99: 1.002196\n",
      "[4.520456259661883, 2.2322500529985194, 1.994183959441724, 1.8741690329240104, 1.7790092767218169, 1.7143712022471524, 1.6656114513564992, 1.6245594511580028, 1.5883660590481588, 1.5555910121660756, 1.5254179537937078, 1.4975635113178574, 1.471862455048581, 1.4482341794926532, 1.4269055954924388, 1.408080092492313, 1.3914501841140188, 1.3762260483775537, 1.361464570013106, 1.3461755781064868, 1.329917323509183, 1.313007366892387, 1.2961687184048023, 1.2804508927892522, 1.2663734691715842, 1.2532867071922444, 1.2398215715707042, 1.225519348309887, 1.2117173523177422, 1.1994413986599957, 1.1892731649703965, 1.1815361480638842, 1.175508418071891, 1.170919702443343, 1.168129879364162, 1.1681843304344013, 1.1094756736285467, 1.0995950921961894, 1.0940971561054809, 1.0905423974916733, 1.0866560297379202, 1.0841921347885353, 1.0812105564894006, 1.0792274213104878, 1.0766962980959691, 1.0748601359965133, 1.0725200457280257, 1.0706006498922962, 1.0683434896758284, 1.0663513879197484, 1.0642090323635318, 1.0622459825896045, 1.0602497020635597, 1.0583660798921315, 1.0564932992469176, 1.0546809296455872, 1.0528834400021125, 1.0511112628575707, 1.0493447434234275, 1.0475840250339317, 1.0458210465567104, 1.0440564812022672, 1.0422884115388997, 1.0405203594211674, 1.0387544598466971, 1.0369964227682922, 1.035250876381299, 1.0335242232580988, 1.0318214660873275, 1.0301472869778299, 1.0285038747203235, 1.0268913111186848, 1.0253068138042303, 1.0237459499014914, 1.0222047150588216, 1.0206844949780538, 1.019193007448722, 1.0177357408147913, 1.0163155947490943, 1.0149348775024165, 1.0135915212112179, 1.0122846538074652, 1.0110181326460972, 1.009802802577989, 1.0086546505545428, 1.0075932815192457, 1.0066405919563293, 1.0058201014807153, 1.0051507149673526, 1.0046311408311066, 1.0042246967897877, 1.0038750868100024, 1.0035434053973125, 1.0032240498493854, 1.0029309360287482, 1.0026804329916443, 1.0024821566468012, 1.002338349313864, 1.002245192716282, 1.0021964102783807]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "# Train on a small subset of the data to see what happens\n",
    "model = RNNVanilla(len(chars))\n",
    "losses = model.train_with_sgd(XTrain[:500], yTrain[:500],break_points_list=[1,10,20,30,40,50,60,70,80,90], \n",
    "                        nepoch=100, evaluate_loss_after=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of hidden units: Try doubling and halving your number of hidden units. And after training, plot the training loss vs the number of training epochs, and show the text sampling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Halving hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reducing hidden layers\n",
    "class RNNVanilla_1:\n",
    "     \n",
    "    def __init__(self, word_dim, hidden_dim=50, bptt_truncate=4):\n",
    "        \n",
    "        # Assign instance variables\n",
    "        self.word_dim = word_dim   #size of the vocabulary\n",
    "        self.hidden_dim = hidden_dim  # size of hidden layer\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        \n",
    "        # Randomly initialize the network parameters\n",
    "        self.U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))\n",
    "        \n",
    "        \n",
    "    def softmax(self,x):\n",
    "        xt = np.exp(x - np.max(x))\n",
    "        return xt / np.sum(xt)\n",
    "    \n",
    "    def forward_propagation(self, x):\n",
    "        # The total number of time steps\n",
    "        T = len(x)\n",
    "\n",
    "        # During forward propagation we save all hidden states in s because need them later.\n",
    "\n",
    "        # We add one additional element for the initial hidden, which we set to 0\n",
    "        s = np.zeros((T + 1, self.hidden_dim))\n",
    "        s[-1] = np.zeros(self.hidden_dim)\n",
    "\n",
    "        # The outputs at each time step. Again, we save them for later.\n",
    "        o = np.zeros((T, self.word_dim))\n",
    "\n",
    "        # For each time step...\n",
    "        for t in np.arange(T):\n",
    "            # Note that we are indxing U by x[t]. This is the same as multiplying U with a one-hot vector.\n",
    "            s[t] = np.tanh(self.U[:,x[t]] + self.W.dot(s[t-1]))\n",
    "            o[t] = self.softmax(self.V.dot(s[t]))\n",
    "        return [o, s] \n",
    "    \n",
    "    def generate_sentence(self, model):  \n",
    "        # We start the sentence with the start token\n",
    "        new_sentence = [(XTrain[10])[0]]\n",
    "\n",
    "        # Repeat until we get an end token\n",
    "        while not new_sentence[-1] == char_to_ix[word_end_token]:\n",
    "            next_word_probs,_ = model.forward_propagation(new_sentence)\n",
    "            sampled_word = char_to_ix[unknown_token]\n",
    "            # We don't want to sample unknown words\n",
    "            while sampled_word == char_to_ix[unknown_token]:\n",
    "                samples = np.random.multinomial(1, next_word_probs[-1])\n",
    "                sampled_word = np.argmax(samples)\n",
    "            new_sentence.append(sampled_word)\n",
    "        sentence_str = [ix_to_char[x] for x in new_sentence[1:-1]]\n",
    "        return sentence_str\n",
    "\n",
    "\n",
    "    def predict(self, x):\n",
    "        # Perform forward propagation and return index of the highest score\n",
    "        o, s = self.forward_propagation(x)\n",
    "        return np.argmax(o, axis=1)\n",
    "\n",
    "    def calculate_total_loss(self, x, y):\n",
    "        L = 0\n",
    "\n",
    "        # For each sentence...\n",
    "        for i in np.arange(len(y)):\n",
    "            o, s = self.forward_propagation(x[i])\n",
    "\n",
    "            # We only care about our prediction of the \"correct\" words\n",
    "            correct_word_predictions = o[np.arange(len(y[i])), y[i]]\n",
    "\n",
    "            # Add to the loss based on how off we were\n",
    "            L += -1 * sum(np.log(correct_word_predictions))\n",
    "        return L\n",
    " \n",
    "    def calculate_loss(self, x, y):\n",
    "        # Divide the total loss by the number of training examples\n",
    "        N = sum((len(y_i) for y_i in y))\n",
    "        return self.calculate_total_loss(x,y)/N\n",
    "    def bptt(self, x, y):\n",
    "        T = len(y)\n",
    "        # Perform forward propagation\n",
    "        o, s = self.forward_propagation(x)\n",
    "        # We accumulate the gradients in these variables\n",
    "        dLdU = np.zeros(self.U.shape)\n",
    "        dLdV = np.zeros(self.V.shape)\n",
    "        dLdW = np.zeros(self.W.shape)\n",
    "        delta_o = o\n",
    "        delta_o[np.arange(len(y)), y] -= 1.\n",
    "        # For each output backwards...\n",
    "        for t in np.arange(T)[::-1]:\n",
    "            dLdV += np.outer(delta_o[t], s[t].T)\n",
    "\n",
    "            # Initial delta calculation\n",
    "            delta_t = self.V.T.dot(delta_o[t]) * (1 - (s[t] ** 2))\n",
    "\n",
    "            # Backpropagation through time (for at most self.bptt_truncate steps)\n",
    "            for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]:\n",
    "\n",
    "                # print \"Backpropagation step t=%d bptt step=%d \" % (t, bptt_step)\n",
    "                dLdW += np.outer(delta_t, s[bptt_step-1])              \n",
    "                dLdU[:,x[bptt_step]] += delta_t\n",
    "\n",
    "                # Update delta for next step\n",
    "                delta_t = self.W.T.dot(delta_t) * (1 - s[bptt_step-1] ** 2)\n",
    "        return [dLdU, dLdV, dLdW]\n",
    "    \n",
    "    def gradient_check(self, x, y, h=0.001, error_threshold=0.01):\n",
    "        # Calculate the gradients using backpropagation. We want to checker if these are correct.\n",
    "        bptt_gradients = self.bptt(x, y)\n",
    "\n",
    "        # List of all parameters we want to check.\n",
    "        model_parameters = ['U', 'V', 'W']\n",
    "\n",
    "        # Gradient check for each parameter\n",
    "        for pidx, pname in enumerate(model_parameters):\n",
    "            # Get the actual parameter value from the mode, e.g. model.W\n",
    "            parameter = operator.attrgetter(pname)(self)\n",
    "            print (\"Performing gradient check for parameter %s with size %d.\" % (pname, np.prod(parameter.shape)))\n",
    "            # Iterate over each element of the parameter matrix, e.g. (0,0), (0,1), ...\n",
    "            it = np.nditer(parameter, flags=['multi_index'], op_flags=['readwrite'])\n",
    "            while not it.finished:\n",
    "                ix = it.multi_index\n",
    "                # Save the original value so we can reset it later\n",
    "                original_value = parameter[ix]\n",
    "                # Estimate the gradient using (f(x+h) - f(x-h))/(2*h)\n",
    "                parameter[ix] = original_value + h\n",
    "                gradplus = self.calculate_total_loss([x],[y])\n",
    "                parameter[ix] = original_value - h\n",
    "                gradminus = self.calculate_total_loss([x],[y])\n",
    "                estimated_gradient = (gradplus - gradminus)/(2*h)\n",
    "                # Reset parameter to original value\n",
    "                parameter[ix] = original_value\n",
    "                # The gradient for this parameter calculated using backpropagation\n",
    "                backprop_gradient = bptt_gradients[pidx][ix]\n",
    "                # calculate The relative error: (|x - y|/(|x| + |y|))\n",
    "                relative_error = np.abs(backprop_gradient - estimated_gradient)/(np.abs(backprop_gradient) + np.abs(estimated_gradient))\n",
    "                # If the error is to large fail the gradient check\n",
    "                if relative_error > error_threshold:\n",
    "                    print (\"Gradient Check ERROR: parameter=%s ix=%s\" % (pname, ix))\n",
    "                    print (\"+h Loss: %f\" % gradplus)\n",
    "                    print (\"-h Loss: %f\" % gradminus)\n",
    "                    print (\"Estimated_gradient: %f\" % estimated_gradient)\n",
    "                    print (\"Backpropagation gradient: %f\" % backprop_gradient)\n",
    "                    print (\"Relative Error: %f\" % relative_error)\n",
    "                    return\n",
    "                it.iternext()\n",
    "            print (\"Gradient check for parameter %s passed.\" % (pname))\n",
    "            \n",
    "            # Performs one step of SGD.\n",
    "    def numpy_sdg_step(self, x, y, learning_rate):\n",
    "        # Calculate the gradients\n",
    "        dLdU, dLdV, dLdW = self.bptt(x, y)\n",
    "        # Change parameters according to gradients and learning rate\n",
    "        self.U -= learning_rate * dLdU\n",
    "        self.V -= learning_rate * dLdV\n",
    "        self.W -= learning_rate * dLdW\n",
    "    def train_with_sgd(self, X_train, y_train, break_points_list, learning_rate=0.005, nepoch=100, evaluate_loss_after=5):\n",
    "        # We keep track of the losses so we can plot them later\n",
    "        losses = []\n",
    "        loss_per_epoch=[]\n",
    "        num_examples_seen = 0\n",
    "        for epoch in range(nepoch):\n",
    "            # Optionally evaluate the loss\n",
    "            if (epoch % evaluate_loss_after == 0):\n",
    "                loss = model.calculate_loss(X_train, y_train)\n",
    "                losses.append((num_examples_seen, loss))\n",
    "                loss_per_epoch.append(loss)\n",
    "                time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                print (\"%s: Loss after num_examples_seen=%d epoch=%d: %f\" % (time, num_examples_seen, epoch, loss))\n",
    "                # Adjust the learning rate if loss increases\n",
    "                if (len(losses) > 1 and losses[-1][1] > losses[-2][1]):\n",
    "                    learning_rate = learning_rate * 0.5 \n",
    "                    print (\"Setting learning rate to %f\" % learning_rate)\n",
    "                sys.stdout.flush()\n",
    "            if epoch in break_points_list:\n",
    "                print(\"========================\")\n",
    "                print(\"Start Pred -- \")\n",
    "                predictions = model.predict(XTrain[10])\n",
    "                print(\"input_characters>\")\n",
    "                print(XTrain[10])\n",
    "                print('%s'%\" \".join([ix_to_char[x] for x in XTrain[10]]))\n",
    "                #print(predictions.shape)\n",
    "                print(\"output_characters>\")\n",
    "                print(predictions)\n",
    "                print('%s'%\" \".join([ix_to_char[x] for x in predictions]))\n",
    "                \n",
    "                num_sentences = 1\n",
    "                senten_min_length = 5\n",
    "\n",
    "                for i in range(num_sentences):\n",
    "                    sent = []\n",
    "                    # We want long sentences, not sentences with one or two words\n",
    "                    while len(sent) < senten_min_length:\n",
    "                        sent = self.generate_sentence(model)\n",
    "                    print(\"Generate Words by feeding the first character to the model at epoch--\",epoch)\n",
    "                    print (\" \".join(sent))\n",
    "                \n",
    "                print(\"========================\")\n",
    "\n",
    "            # For each training example...\n",
    "            for i in range(len(y_train)):\n",
    "                # One SGD step\n",
    "                model.numpy_sdg_step(X_train[i], y_train[i], learning_rate)\n",
    "                num_examples_seen += 1\n",
    "            #Plot the Loss Curves\n",
    "        print(loss_per_epoch)\n",
    "        plt.figure(figsize=[8,6])\n",
    "        plt.plot(loss_per_epoch,'r',linewidth=3.0)\n",
    "        plt.legend(['Training loss'],fontsize=18)\n",
    "        plt.xlabel('Epochs ',fontsize=16)\n",
    "        plt.ylabel('Loss',fontsize=16)\n",
    "        plt.title('Loss Curves',fontsize=16)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-06 23:25:39: Loss after num_examples_seen=0 epoch=0: 1.002185\n",
      "2019-11-06 23:25:41: Loss after num_examples_seen=500 epoch=1: 1.334171\n",
      "Setting learning rate to 0.002500\n",
      "2019-11-06 23:25:42: Loss after num_examples_seen=1000 epoch=2: 1.050904\n",
      "2019-11-06 23:25:44: Loss after num_examples_seen=1500 epoch=3: 1.020802\n",
      "2019-11-06 23:25:45: Loss after num_examples_seen=2000 epoch=4: 1.013143\n",
      "2019-11-06 23:25:47: Loss after num_examples_seen=2500 epoch=5: 0.978995\n",
      "2019-11-06 23:25:48: Loss after num_examples_seen=3000 epoch=6: 0.973052\n",
      "2019-11-06 23:25:49: Loss after num_examples_seen=3500 epoch=7: 0.971488\n",
      "2019-11-06 23:25:50: Loss after num_examples_seen=4000 epoch=8: 0.968718\n",
      "2019-11-06 23:25:51: Loss after num_examples_seen=4500 epoch=9: 0.969388\n",
      "Setting learning rate to 0.001250\n",
      "2019-11-06 23:25:52: Loss after num_examples_seen=5000 epoch=10: 0.926892\n",
      "========================\n",
      "Start Pred -- \n",
      "input_characters>\n",
      "[5, 36, 64, 47, 39, 12, 10, 36]\n",
      "S   e a r t h  \n",
      "output_characters>\n",
      "[36  6 47 39 12 10 36 60]\n",
      "  c a r t h   E\n",
      "Generate Words by feeding the first character to the model at epoch-- 10\n",
      "  c h a p t e r  \n",
      "========================\n",
      "2019-11-06 23:25:54: Loss after num_examples_seen=5500 epoch=11: 0.923802\n",
      "2019-11-06 23:25:55: Loss after num_examples_seen=6000 epoch=12: 0.920715\n",
      "2019-11-06 23:25:56: Loss after num_examples_seen=6500 epoch=13: 0.917779\n",
      "2019-11-06 23:25:57: Loss after num_examples_seen=7000 epoch=14: 0.915136\n",
      "2019-11-06 23:25:58: Loss after num_examples_seen=7500 epoch=15: 0.912750\n",
      "2019-11-06 23:26:00: Loss after num_examples_seen=8000 epoch=16: 0.910586\n",
      "2019-11-06 23:26:02: Loss after num_examples_seen=8500 epoch=17: 0.908610\n",
      "2019-11-06 23:26:03: Loss after num_examples_seen=9000 epoch=18: 0.906796\n",
      "2019-11-06 23:26:04: Loss after num_examples_seen=9500 epoch=19: 0.905123\n",
      "2019-11-06 23:26:06: Loss after num_examples_seen=10000 epoch=20: 0.903576\n",
      "========================\n",
      "Start Pred -- \n",
      "input_characters>\n",
      "[5, 36, 64, 47, 39, 12, 10, 36]\n",
      "S   e a r t h  \n",
      "output_characters>\n",
      "[36  6 47 39 12 10 36 60]\n",
      "  c a r t h   E\n",
      "Generate Words by feeding the first character to the model at epoch-- 20\n",
      "  j o u r n e y  \n",
      "========================\n",
      "2019-11-06 23:26:07: Loss after num_examples_seen=10500 epoch=21: 0.902142\n",
      "2019-11-06 23:26:08: Loss after num_examples_seen=11000 epoch=22: 0.900811\n",
      "2019-11-06 23:26:09: Loss after num_examples_seen=11500 epoch=23: 0.899575\n",
      "2019-11-06 23:26:11: Loss after num_examples_seen=12000 epoch=24: 0.898427\n",
      "2019-11-06 23:26:12: Loss after num_examples_seen=12500 epoch=25: 0.897358\n",
      "2019-11-06 23:26:13: Loss after num_examples_seen=13000 epoch=26: 0.896362\n",
      "2019-11-06 23:26:14: Loss after num_examples_seen=13500 epoch=27: 0.895431\n",
      "2019-11-06 23:26:16: Loss after num_examples_seen=14000 epoch=28: 0.894560\n",
      "2019-11-06 23:26:17: Loss after num_examples_seen=14500 epoch=29: 0.893743\n",
      "2019-11-06 23:26:18: Loss after num_examples_seen=15000 epoch=30: 0.892977\n",
      "========================\n",
      "Start Pred -- \n",
      "input_characters>\n",
      "[5, 36, 64, 47, 39, 12, 10, 36]\n",
      "S   e a r t h  \n",
      "output_characters>\n",
      "[36  6 47 39 12 10 36 60]\n",
      "  c a r t h   E\n",
      "Generate Words by feeding the first character to the model at epoch-- 30\n",
      "  w 8 o n e  \n",
      "========================\n",
      "2019-11-06 23:26:19: Loss after num_examples_seen=15500 epoch=31: 0.892256\n",
      "2019-11-06 23:26:20: Loss after num_examples_seen=16000 epoch=32: 0.891579\n",
      "2019-11-06 23:26:21: Loss after num_examples_seen=16500 epoch=33: 0.890942\n",
      "2019-11-06 23:26:22: Loss after num_examples_seen=17000 epoch=34: 0.890344\n",
      "2019-11-06 23:26:24: Loss after num_examples_seen=17500 epoch=35: 0.889782\n",
      "2019-11-06 23:26:25: Loss after num_examples_seen=18000 epoch=36: 0.889255\n",
      "2019-11-06 23:26:26: Loss after num_examples_seen=18500 epoch=37: 0.888763\n",
      "2019-11-06 23:26:27: Loss after num_examples_seen=19000 epoch=38: 0.888302\n",
      "2019-11-06 23:26:28: Loss after num_examples_seen=19500 epoch=39: 0.887873\n",
      "2019-11-06 23:26:29: Loss after num_examples_seen=20000 epoch=40: 0.887474\n",
      "========================\n",
      "Start Pred -- \n",
      "input_characters>\n",
      "[5, 36, 64, 47, 39, 12, 10, 36]\n",
      "S   e a r t h  \n",
      "output_characters>\n",
      "[36  6 47 39 12 10 36 60]\n",
      "  c a r t h   E\n",
      "Generate Words by feeding the first character to the model at epoch-- 40\n",
      "  t h e  \n",
      "========================\n",
      "2019-11-06 23:26:30: Loss after num_examples_seen=20500 epoch=41: 0.887104\n",
      "2019-11-06 23:26:31: Loss after num_examples_seen=21000 epoch=42: 0.886760\n",
      "2019-11-06 23:26:33: Loss after num_examples_seen=21500 epoch=43: 0.886442\n",
      "2019-11-06 23:26:34: Loss after num_examples_seen=22000 epoch=44: 0.886146\n",
      "2019-11-06 23:26:35: Loss after num_examples_seen=22500 epoch=45: 0.885871\n",
      "2019-11-06 23:26:36: Loss after num_examples_seen=23000 epoch=46: 0.885615\n",
      "2019-11-06 23:26:37: Loss after num_examples_seen=23500 epoch=47: 0.885374\n",
      "2019-11-06 23:26:38: Loss after num_examples_seen=24000 epoch=48: 0.885146\n",
      "2019-11-06 23:26:40: Loss after num_examples_seen=24500 epoch=49: 0.884928\n",
      "2019-11-06 23:26:41: Loss after num_examples_seen=25000 epoch=50: 0.884719\n",
      "========================\n",
      "Start Pred -- \n",
      "input_characters>\n",
      "[5, 36, 64, 47, 39, 12, 10, 36]\n",
      "S   e a r t h  \n",
      "output_characters>\n",
      "[36  6 47 39 12 10 36 60]\n",
      "  c a r t h   E\n",
      "Generate Words by feeding the first character to the model at epoch-- 50\n",
      "  c h a p t e r  \n",
      "========================\n",
      "2019-11-06 23:26:42: Loss after num_examples_seen=25500 epoch=51: 0.884514\n",
      "2019-11-06 23:26:43: Loss after num_examples_seen=26000 epoch=52: 0.884313\n",
      "2019-11-06 23:26:44: Loss after num_examples_seen=26500 epoch=53: 0.884114\n",
      "2019-11-06 23:26:45: Loss after num_examples_seen=27000 epoch=54: 0.883914\n",
      "2019-11-06 23:26:46: Loss after num_examples_seen=27500 epoch=55: 0.883714\n",
      "2019-11-06 23:26:48: Loss after num_examples_seen=28000 epoch=56: 0.883511\n",
      "2019-11-06 23:26:49: Loss after num_examples_seen=28500 epoch=57: 0.883306\n",
      "2019-11-06 23:26:50: Loss after num_examples_seen=29000 epoch=58: 0.883098\n",
      "2019-11-06 23:26:51: Loss after num_examples_seen=29500 epoch=59: 0.882886\n",
      "2019-11-06 23:26:52: Loss after num_examples_seen=30000 epoch=60: 0.882671\n",
      "========================\n",
      "Start Pred -- \n",
      "input_characters>\n",
      "[5, 36, 64, 47, 39, 12, 10, 36]\n",
      "S   e a r t h  \n",
      "output_characters>\n",
      "[36  6 47 39 12 10 36 60]\n",
      "  c a r t h   E\n",
      "Generate Words by feeding the first character to the model at epoch-- 60\n",
      "  v o y a g e  \n",
      "========================\n",
      "2019-11-06 23:26:53: Loss after num_examples_seen=30500 epoch=61: 0.882452\n",
      "2019-11-06 23:26:54: Loss after num_examples_seen=31000 epoch=62: 0.882232\n",
      "2019-11-06 23:26:56: Loss after num_examples_seen=31500 epoch=63: 0.882009\n",
      "2019-11-06 23:26:57: Loss after num_examples_seen=32000 epoch=64: 0.881785\n",
      "2019-11-06 23:26:58: Loss after num_examples_seen=32500 epoch=65: 0.881562\n",
      "2019-11-06 23:26:59: Loss after num_examples_seen=33000 epoch=66: 0.881340\n",
      "2019-11-06 23:27:00: Loss after num_examples_seen=33500 epoch=67: 0.881120\n",
      "2019-11-06 23:27:01: Loss after num_examples_seen=34000 epoch=68: 0.880904\n",
      "2019-11-06 23:27:03: Loss after num_examples_seen=34500 epoch=69: 0.880694\n",
      "2019-11-06 23:27:04: Loss after num_examples_seen=35000 epoch=70: 0.880490\n",
      "========================\n",
      "Start Pred -- \n",
      "input_characters>\n",
      "[5, 36, 64, 47, 39, 12, 10, 36]\n",
      "S   e a r t h  \n",
      "output_characters>\n",
      "[36  6 47 39 12 10 36 60]\n",
      "  c a r t h   E\n",
      "Generate Words by feeding the first character to the model at epoch-- 70\n",
      "  g a l l e r  \n",
      "========================\n",
      "2019-11-06 23:27:05: Loss after num_examples_seen=35500 epoch=71: 0.880295\n",
      "2019-11-06 23:27:06: Loss after num_examples_seen=36000 epoch=72: 0.880109\n",
      "2019-11-06 23:27:07: Loss after num_examples_seen=36500 epoch=73: 0.879934\n",
      "2019-11-06 23:27:08: Loss after num_examples_seen=37000 epoch=74: 0.879771\n",
      "2019-11-06 23:27:09: Loss after num_examples_seen=37500 epoch=75: 0.879620\n",
      "2019-11-06 23:27:11: Loss after num_examples_seen=38000 epoch=76: 0.879483\n",
      "2019-11-06 23:27:12: Loss after num_examples_seen=38500 epoch=77: 0.879359\n",
      "2019-11-06 23:27:13: Loss after num_examples_seen=39000 epoch=78: 0.879248\n",
      "2019-11-06 23:27:14: Loss after num_examples_seen=39500 epoch=79: 0.879151\n",
      "2019-11-06 23:27:15: Loss after num_examples_seen=40000 epoch=80: 0.879068\n",
      "========================\n",
      "Start Pred -- \n",
      "input_characters>\n",
      "[5, 36, 64, 47, 39, 12, 10, 36]\n",
      "S   e a r t h  \n",
      "output_characters>\n",
      "[36  6 47 39 12 10 36 60]\n",
      "  c a r t h   E\n",
      "Generate Words by feeding the first character to the model at epoch-- 80\n",
      "  w h i s c e n v e r e  \n",
      "========================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-06 23:27:16: Loss after num_examples_seen=40500 epoch=81: 0.878997\n",
      "2019-11-06 23:27:17: Loss after num_examples_seen=41000 epoch=82: 0.878939\n",
      "2019-11-06 23:27:19: Loss after num_examples_seen=41500 epoch=83: 0.878892\n",
      "2019-11-06 23:27:20: Loss after num_examples_seen=42000 epoch=84: 0.878855\n",
      "2019-11-06 23:27:21: Loss after num_examples_seen=42500 epoch=85: 0.878829\n",
      "2019-11-06 23:27:22: Loss after num_examples_seen=43000 epoch=86: 0.878810\n",
      "2019-11-06 23:27:23: Loss after num_examples_seen=43500 epoch=87: 0.878800\n",
      "2019-11-06 23:27:24: Loss after num_examples_seen=44000 epoch=88: 0.878797\n",
      "2019-11-06 23:27:25: Loss after num_examples_seen=44500 epoch=89: 0.878799\n",
      "Setting learning rate to 0.000625\n",
      "2019-11-06 23:27:27: Loss after num_examples_seen=45000 epoch=90: 0.838910\n",
      "========================\n",
      "Start Pred -- \n",
      "input_characters>\n",
      "[5, 36, 64, 47, 39, 12, 10, 36]\n",
      "S   e a r t h  \n",
      "output_characters>\n",
      "[36  6 47 39 12 10 36 60]\n",
      "  c a r t h   E\n",
      "Generate Words by feeding the first character to the model at epoch-- 90\n",
      "  s t a r t  \n",
      "========================\n",
      "2019-11-06 23:27:28: Loss after num_examples_seen=45500 epoch=91: 0.834390\n",
      "2019-11-06 23:27:29: Loss after num_examples_seen=46000 epoch=92: 0.831891\n",
      "2019-11-06 23:27:30: Loss after num_examples_seen=46500 epoch=93: 0.829914\n",
      "2019-11-06 23:27:31: Loss after num_examples_seen=47000 epoch=94: 0.828270\n",
      "2019-11-06 23:27:32: Loss after num_examples_seen=47500 epoch=95: 0.826868\n",
      "2019-11-06 23:27:33: Loss after num_examples_seen=48000 epoch=96: 0.825647\n",
      "2019-11-06 23:27:35: Loss after num_examples_seen=48500 epoch=97: 0.824567\n",
      "2019-11-06 23:27:36: Loss after num_examples_seen=49000 epoch=98: 0.823600\n",
      "2019-11-06 23:27:37: Loss after num_examples_seen=49500 epoch=99: 0.822726\n",
      "[1.0021851719530868, 1.3341713645246576, 1.0509040636771523, 1.020801860592283, 1.0131427887900755, 0.9789945298537671, 0.9730516012017573, 0.9714878330705217, 0.9687176306318471, 0.9693876248234864, 0.9268920170352721, 0.9238015815083099, 0.9207154963427445, 0.9177794344013761, 0.91513612674183, 0.9127502858175905, 0.9105859153858626, 0.9086098425332545, 0.9067957555797012, 0.9051230677994861, 0.9035758378442584, 0.902141726499738, 0.9008109445047039, 0.8995751795410862, 0.898426707662259, 0.8973579735674171, 0.8963616672256334, 0.8954310128904797, 0.8945599837305457, 0.8937433629642524, 0.8929767082238629, 0.8922562807220049, 0.8915789644626407, 0.8909421802980617, 0.8903437960622147, 0.8897820349235608, 0.889255384233471, 0.8887625066988549, 0.8883021554421696, 0.8878730943409485, 0.8874740247743247, 0.8871035199630936, 0.8867599692327416, 0.8864415368829441, 0.8861461426587117, 0.8858714708641121, 0.8856150113418687, 0.8853741285422274, 0.8851461478385793, 0.8849284447782836, 0.8847185246737632, 0.8845140852992925, 0.8843130612365162, 0.8841136521219728, 0.8839143381406571, 0.883713885474958, 0.8835113434153804, 0.8833060345845699, 0.8830975405214834, 0.8828856859204567, 0.8826705248092419, 0.8824523302679671, 0.882231586682012, 0.8820089814859353, 0.8817853929329919, 0.8815618715336215, 0.88133961455561, 0.8811199344306975, 0.8809042226087808, 0.8806939104134544, 0.8804904281408554, 0.8802951633821013, 0.8801094195589415, 0.8799343759681213, 0.8797710510970924, 0.879620271374146, 0.8794826476077796, 0.8793585609706697, 0.8792481594619169, 0.8791513645450492, 0.8790678865557479, 0.8789972469787105, 0.8789388059400348, 0.8788917938323889, 0.8788553462026744, 0.8788285405502273, 0.8788104328241714, 0.8788000908406169, 0.8787966220261307, 0.8787991938318478, 0.8389104948404176, 0.8343897601924297, 0.8318912354389377, 0.829914330537884, 0.828270474144658, 0.8268677484863663, 0.8256466435455575, 0.8245669314056863, 0.8236003059969184, 0.8227260029093664]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "# Train on a small subset of the data to see what happens\n",
    "model1 = RNNVanilla_1(len(chars))\n",
    "losses = model1.train_with_sgd(XTrain[:500], yTrain[:500],break_points_list=[10,20,30,40,50,60,70,80,90],\n",
    "                              nepoch=100, learning_rate=0.005, evaluate_loss_after=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gradient check after halving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing gradient check for parameter U with size 1000.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pprusty05/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:138: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check for parameter U passed.\n",
      "Performing gradient check for parameter V with size 1000.\n",
      "Gradient check for parameter V passed.\n",
      "Performing gradient check for parameter W with size 100.\n",
      "Gradient check for parameter W passed.\n"
     ]
    }
   ],
   "source": [
    "# To avoid performing millions of expensive calculations we use a smaller vocabulary size for checking.\n",
    "grad_check_vocab_size = 100\n",
    "np.random.seed(10)\n",
    "model1 = RNNVanilla_1(grad_check_vocab_size, 10, bptt_truncate=1000)\n",
    "model1.gradient_check([0,1,2,3], [1,2,3,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doubling hidden units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doubling hidden layers\n",
    "class RNNVanilla_2:\n",
    "     \n",
    "    def __init__(self, word_dim, hidden_dim=200, bptt_truncate=4):\n",
    "        \n",
    "        # Assign instance variables\n",
    "        self.word_dim = word_dim   #size of the vocabulary\n",
    "        self.hidden_dim = hidden_dim  # size of hidden layer\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        \n",
    "        # Randomly initialize the network parameters\n",
    "        self.U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))\n",
    "        \n",
    "        \n",
    "    def softmax(self,x):\n",
    "        xt = np.exp(x - np.max(x))\n",
    "        return xt / np.sum(xt)\n",
    "    \n",
    "    def forward_propagation(self, x):\n",
    "        # The total number of time steps\n",
    "        T = len(x)\n",
    "\n",
    "        # During forward propagation we save all hidden states in s because need them later.\n",
    "\n",
    "        # We add one additional element for the initial hidden, which we set to 0\n",
    "        s = np.zeros((T + 1, self.hidden_dim))\n",
    "        s[-1] = np.zeros(self.hidden_dim)\n",
    "\n",
    "        # The outputs at each time step. Again, we save them for later.\n",
    "        o = np.zeros((T, self.word_dim))\n",
    "\n",
    "        # For each time step...\n",
    "        for t in np.arange(T):\n",
    "            # Note that we are indxing U by x[t]. This is the same as multiplying U with a one-hot vector.\n",
    "            s[t] = np.tanh(self.U[:,x[t]] + self.W.dot(s[t-1]))\n",
    "            o[t] = self.softmax(self.V.dot(s[t]))\n",
    "        return [o, s] \n",
    "\n",
    "    def predict(self, x):\n",
    "        # Perform forward propagation and return index of the highest score\n",
    "        o, s = self.forward_propagation(x)\n",
    "        return np.argmax(o, axis=1)\n",
    "\n",
    "    def generate_sentence(self, model):  \n",
    "        # We start the sentence with the start token\n",
    "        new_sentence = [(XTrain[10])[0]]\n",
    "\n",
    "        # Repeat until we get an end token\n",
    "        while not new_sentence[-1] == char_to_ix[word_end_token]:\n",
    "            next_word_probs,_ = model.forward_propagation(new_sentence)\n",
    "            sampled_word = char_to_ix[unknown_token]\n",
    "            # We don't want to sample unknown words\n",
    "            while sampled_word == char_to_ix[unknown_token]:\n",
    "                samples = np.random.multinomial(1, next_word_probs[-1])\n",
    "                sampled_word = np.argmax(samples)\n",
    "            new_sentence.append(sampled_word)\n",
    "        sentence_str = [ix_to_char[x] for x in new_sentence[1:-1]]\n",
    "        return sentence_str\n",
    "\n",
    "    def calculate_total_loss(self, x, y):\n",
    "        L = 0\n",
    "\n",
    "        # For each sentence...\n",
    "        for i in np.arange(len(y)):\n",
    "            o, s = self.forward_propagation(x[i])\n",
    "\n",
    "            # We only care about our prediction of the \"correct\" words\n",
    "            correct_word_predictions = o[np.arange(len(y[i])), y[i]]\n",
    "\n",
    "            # Add to the loss based on how off we were\n",
    "            L += -1 * sum(np.log(correct_word_predictions))\n",
    "        return L\n",
    " \n",
    "    def calculate_loss(self, x, y):\n",
    "        # Divide the total loss by the number of training examples\n",
    "        N = sum((len(y_i) for y_i in y))\n",
    "        return self.calculate_total_loss(x,y)/N\n",
    "    def bptt(self, x, y):\n",
    "        T = len(y)\n",
    "        # Perform forward propagation\n",
    "        o, s = self.forward_propagation(x)\n",
    "        # We accumulate the gradients in these variables\n",
    "        dLdU = np.zeros(self.U.shape)\n",
    "        dLdV = np.zeros(self.V.shape)\n",
    "        dLdW = np.zeros(self.W.shape)\n",
    "        delta_o = o\n",
    "        delta_o[np.arange(len(y)), y] -= 1.\n",
    "        # For each output backwards...\n",
    "        for t in np.arange(T)[::-1]:\n",
    "            dLdV += np.outer(delta_o[t], s[t].T)\n",
    "\n",
    "            # Initial delta calculation\n",
    "            delta_t = self.V.T.dot(delta_o[t]) * (1 - (s[t] ** 2))\n",
    "\n",
    "            # Backpropagation through time (for at most self.bptt_truncate steps)\n",
    "            for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]:\n",
    "\n",
    "                # print \"Backpropagation step t=%d bptt step=%d \" % (t, bptt_step)\n",
    "                dLdW += np.outer(delta_t, s[bptt_step-1])              \n",
    "                dLdU[:,x[bptt_step]] += delta_t\n",
    "\n",
    "                # Update delta for next step\n",
    "                delta_t = self.W.T.dot(delta_t) * (1 - s[bptt_step-1] ** 2)\n",
    "        return [dLdU, dLdV, dLdW]\n",
    "    \n",
    "    def gradient_check(self, x, y, h=0.001, error_threshold=0.01):\n",
    "        # Calculate the gradients using backpropagation. We want to checker if these are correct.\n",
    "        bptt_gradients = self.bptt(x, y)\n",
    "\n",
    "        # List of all parameters we want to check.\n",
    "        model_parameters = ['U', 'V', 'W']\n",
    "\n",
    "        # Gradient check for each parameter\n",
    "        for pidx, pname in enumerate(model_parameters):\n",
    "            # Get the actual parameter value from the mode, e.g. model.W\n",
    "            parameter = operator.attrgetter(pname)(self)\n",
    "            print (\"Performing gradient check for parameter %s with size %d.\" % (pname, np.prod(parameter.shape)))\n",
    "            # Iterate over each element of the parameter matrix, e.g. (0,0), (0,1), ...\n",
    "            it = np.nditer(parameter, flags=['multi_index'], op_flags=['readwrite'])\n",
    "            while not it.finished:\n",
    "                ix = it.multi_index\n",
    "                # Save the original value so we can reset it later\n",
    "                original_value = parameter[ix]\n",
    "                # Estimate the gradient using (f(x+h) - f(x-h))/(2*h)\n",
    "                parameter[ix] = original_value + h\n",
    "                gradplus = self.calculate_total_loss([x],[y])\n",
    "                parameter[ix] = original_value - h\n",
    "                gradminus = self.calculate_total_loss([x],[y])\n",
    "                estimated_gradient = (gradplus - gradminus)/(2*h)\n",
    "                # Reset parameter to original value\n",
    "                parameter[ix] = original_value\n",
    "                # The gradient for this parameter calculated using backpropagation\n",
    "                backprop_gradient = bptt_gradients[pidx][ix]\n",
    "                # calculate The relative error: (|x - y|/(|x| + |y|))\n",
    "                relative_error = np.abs(backprop_gradient - estimated_gradient)/(np.abs(backprop_gradient) + np.abs(estimated_gradient))\n",
    "                # If the error is to large fail the gradient check\n",
    "                if relative_error > error_threshold:\n",
    "                    print (\"Gradient Check ERROR: parameter=%s ix=%s\" % (pname, ix))\n",
    "                    print (\"+h Loss: %f\" % gradplus)\n",
    "                    print (\"-h Loss: %f\" % gradminus)\n",
    "                    print (\"Estimated_gradient: %f\" % estimated_gradient)\n",
    "                    print (\"Backpropagation gradient: %f\" % backprop_gradient)\n",
    "                    print (\"Relative Error: %f\" % relative_error)\n",
    "                    return\n",
    "                it.iternext()\n",
    "            print (\"Gradient check for parameter %s passed.\" % (pname))\n",
    "            \n",
    "            # Performs one step of SGD.\n",
    "    def numpy_sdg_step(self, x, y, learning_rate):\n",
    "        # Calculate the gradients\n",
    "        dLdU, dLdV, dLdW = self.bptt(x, y)\n",
    "        # Change parameters according to gradients and learning rate\n",
    "        self.U -= learning_rate * dLdU\n",
    "        self.V -= learning_rate * dLdV\n",
    "        self.W -= learning_rate * dLdW\n",
    "    def train_with_sgd(self, X_train, y_train, break_points_list, learning_rate=0.005, nepoch=100, evaluate_loss_after=5):\n",
    "        # We keep track of the losses so we can plot them later\n",
    "        losses = []\n",
    "        loss_per_epoch=[]\n",
    "        num_examples_seen = 0\n",
    "        for epoch in range(nepoch):\n",
    "            # Optionally evaluate the loss\n",
    "            if (epoch % evaluate_loss_after == 0):\n",
    "                loss = model.calculate_loss(X_train, y_train)\n",
    "                losses.append((num_examples_seen, loss))\n",
    "                loss_per_epoch.append(loss)\n",
    "                time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                print (\"%s: Loss after num_examples_seen=%d epoch=%d: %f\" % (time, num_examples_seen, epoch, loss))\n",
    "                # Adjust the learning rate if loss increases\n",
    "                if (len(losses) > 1 and losses[-1][1] > losses[-2][1]):\n",
    "                    learning_rate = learning_rate * 0.5 \n",
    "                    print (\"Setting learning rate to %f\" % learning_rate)\n",
    "                sys.stdout.flush()\n",
    "            if epoch in break_points_list:\n",
    "                print(\"========================\")\n",
    "                print(\"Start Pred -- \")\n",
    "                predictions = model.predict(XTrain[10])\n",
    "                print(\"input_characters>\")\n",
    "                print(XTrain[10])\n",
    "                print('%s'%\" \".join([ix_to_char[x] for x in XTrain[10]]))\n",
    "                #print(predictions.shape)\n",
    "                print(\"output_characters>\")\n",
    "                print(predictions)\n",
    "                print('%s'%\" \".join([ix_to_char[x] for x in predictions]))\n",
    "                \n",
    "                num_sentences = 1\n",
    "                senten_min_length = 5\n",
    "\n",
    "                for i in range(num_sentences):\n",
    "                    sent = []\n",
    "                    # We want long sentences, not sentences with one or two words\n",
    "                    while len(sent) < senten_min_length:\n",
    "                        sent = self.generate_sentence(model)\n",
    "                    print(\"Generate Words by feeding the first character to the model at epoch--\",epoch)\n",
    "                    print (\" \".join(sent))\n",
    "                \n",
    "                print(\"========================\")\n",
    "            # For each training example...\n",
    "            for i in range(len(y_train)):\n",
    "                # One SGD step\n",
    "                model.numpy_sdg_step(X_train[i], y_train[i], learning_rate)\n",
    "                num_examples_seen += 1\n",
    "            #Plot the Loss Curves\n",
    "        print(loss_per_epoch)\n",
    "        plt.figure(figsize=[8,6])\n",
    "        plt.plot(loss_per_epoch,'r',linewidth=3.0)\n",
    "        plt.legend(['Training loss'],fontsize=18)\n",
    "        plt.xlabel('Epochs ',fontsize=16)\n",
    "        plt.ylabel('Loss',fontsize=16)\n",
    "        plt.title('Loss Curves',fontsize=16)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-06 23:32:37: Loss after num_examples_seen=0 epoch=0: 0.844875\n",
      "2019-11-06 23:32:39: Loss after num_examples_seen=500 epoch=1: 1.169604\n",
      "Setting learning rate to 0.002500\n",
      "2019-11-06 23:32:40: Loss after num_examples_seen=1000 epoch=2: 0.970497\n",
      "2019-11-06 23:32:42: Loss after num_examples_seen=1500 epoch=3: 0.900884\n",
      "2019-11-06 23:32:44: Loss after num_examples_seen=2000 epoch=4: 0.895564\n",
      "2019-11-06 23:32:45: Loss after num_examples_seen=2500 epoch=5: 0.889126\n",
      "2019-11-06 23:32:47: Loss after num_examples_seen=3000 epoch=6: 0.887289\n",
      "2019-11-06 23:32:48: Loss after num_examples_seen=3500 epoch=7: 0.886229\n",
      "2019-11-06 23:32:50: Loss after num_examples_seen=4000 epoch=8: 0.886207\n",
      "2019-11-06 23:32:52: Loss after num_examples_seen=4500 epoch=9: 0.886897\n",
      "Setting learning rate to 0.001250\n",
      "2019-11-06 23:32:54: Loss after num_examples_seen=5000 epoch=10: 0.838184\n",
      "========================\n",
      "Start Pred -- \n",
      "input_characters>\n",
      "[5, 36, 64, 47, 39, 12, 10, 36]\n",
      "S   e a r t h  \n",
      "output_characters>\n",
      "[36  6 47 69 12 10 36 60]\n",
      "  c a s t h   E\n",
      "Generate Words by feeding the first character to the model at epoch-- 10\n",
      "  c h a p t e r  \n",
      "========================\n",
      "2019-11-06 23:32:56: Loss after num_examples_seen=5500 epoch=11: 0.838180\n",
      "2019-11-06 23:32:57: Loss after num_examples_seen=6000 epoch=12: 0.837611\n",
      "2019-11-06 23:32:59: Loss after num_examples_seen=6500 epoch=13: 0.837160\n",
      "2019-11-06 23:33:00: Loss after num_examples_seen=7000 epoch=14: 0.836836\n",
      "2019-11-06 23:33:02: Loss after num_examples_seen=7500 epoch=15: 0.836615\n",
      "2019-11-06 23:33:03: Loss after num_examples_seen=8000 epoch=16: 0.836470\n",
      "2019-11-06 23:33:05: Loss after num_examples_seen=8500 epoch=17: 0.836381\n",
      "2019-11-06 23:33:07: Loss after num_examples_seen=9000 epoch=18: 0.836330\n",
      "2019-11-06 23:33:09: Loss after num_examples_seen=9500 epoch=19: 0.836305\n",
      "2019-11-06 23:33:10: Loss after num_examples_seen=10000 epoch=20: 0.836297\n",
      "========================\n",
      "Start Pred -- \n",
      "input_characters>\n",
      "[5, 36, 64, 47, 39, 12, 10, 36]\n",
      "S   e a r t h  \n",
      "output_characters>\n",
      "[36  6 47 69 12 10 36 60]\n",
      "  c a s t h   E\n",
      "Generate Words by feeding the first character to the model at epoch-- 20\n",
      "  o u r c h a  \n",
      "========================\n",
      "2019-11-06 23:33:12: Loss after num_examples_seen=10500 epoch=21: 0.836300\n",
      "Setting learning rate to 0.000625\n",
      "2019-11-06 23:33:13: Loss after num_examples_seen=11000 epoch=22: 0.802995\n",
      "2019-11-06 23:33:15: Loss after num_examples_seen=11500 epoch=23: 0.800893\n",
      "2019-11-06 23:33:16: Loss after num_examples_seen=12000 epoch=24: 0.799209\n",
      "2019-11-06 23:33:18: Loss after num_examples_seen=12500 epoch=25: 0.797788\n",
      "2019-11-06 23:33:19: Loss after num_examples_seen=13000 epoch=26: 0.796571\n",
      "2019-11-06 23:33:20: Loss after num_examples_seen=13500 epoch=27: 0.795509\n",
      "2019-11-06 23:33:22: Loss after num_examples_seen=14000 epoch=28: 0.794570\n",
      "2019-11-06 23:33:23: Loss after num_examples_seen=14500 epoch=29: 0.793728\n",
      "2019-11-06 23:33:24: Loss after num_examples_seen=15000 epoch=30: 0.792963\n",
      "========================\n",
      "Start Pred -- \n",
      "input_characters>\n",
      "[5, 36, 64, 47, 39, 12, 10, 36]\n",
      "S   e a r t h  \n",
      "output_characters>\n",
      "[36  6 47 69 12 10 36 60]\n",
      "  c a s t h   E\n",
      "Generate Words by feeding the first character to the model at epoch-- 30\n",
      "  c h a p t e r  \n",
      "========================\n",
      "2019-11-06 23:33:25: Loss after num_examples_seen=15500 epoch=31: 0.792262\n",
      "2019-11-06 23:33:26: Loss after num_examples_seen=16000 epoch=32: 0.791612\n",
      "2019-11-06 23:33:28: Loss after num_examples_seen=16500 epoch=33: 0.791003\n",
      "2019-11-06 23:33:29: Loss after num_examples_seen=17000 epoch=34: 0.790429\n",
      "2019-11-06 23:33:30: Loss after num_examples_seen=17500 epoch=35: 0.789884\n",
      "2019-11-06 23:33:31: Loss after num_examples_seen=18000 epoch=36: 0.789364\n",
      "2019-11-06 23:33:32: Loss after num_examples_seen=18500 epoch=37: 0.788864\n",
      "2019-11-06 23:33:33: Loss after num_examples_seen=19000 epoch=38: 0.788381\n",
      "2019-11-06 23:33:35: Loss after num_examples_seen=19500 epoch=39: 0.787915\n",
      "2019-11-06 23:33:36: Loss after num_examples_seen=20000 epoch=40: 0.787462\n",
      "========================\n",
      "Start Pred -- \n",
      "input_characters>\n",
      "[5, 36, 64, 47, 39, 12, 10, 36]\n",
      "S   e a r t h  \n",
      "output_characters>\n",
      "[36  6 47 39 12 10 36 60]\n",
      "  c a r t h   E\n",
      "Generate Words by feeding the first character to the model at epoch-- 40\n",
      "  t h e  \n",
      "========================\n",
      "2019-11-06 23:33:37: Loss after num_examples_seen=20500 epoch=41: 0.787021\n",
      "2019-11-06 23:33:38: Loss after num_examples_seen=21000 epoch=42: 0.786591\n",
      "2019-11-06 23:33:39: Loss after num_examples_seen=21500 epoch=43: 0.786171\n",
      "2019-11-06 23:33:40: Loss after num_examples_seen=22000 epoch=44: 0.785761\n",
      "2019-11-06 23:33:41: Loss after num_examples_seen=22500 epoch=45: 0.785359\n",
      "2019-11-06 23:33:43: Loss after num_examples_seen=23000 epoch=46: 0.784966\n",
      "2019-11-06 23:33:44: Loss after num_examples_seen=23500 epoch=47: 0.784580\n",
      "2019-11-06 23:33:45: Loss after num_examples_seen=24000 epoch=48: 0.784201\n",
      "2019-11-06 23:33:46: Loss after num_examples_seen=24500 epoch=49: 0.783830\n",
      "2019-11-06 23:33:47: Loss after num_examples_seen=25000 epoch=50: 0.783465\n",
      "========================\n",
      "Start Pred -- \n",
      "input_characters>\n",
      "[5, 36, 64, 47, 39, 12, 10, 36]\n",
      "S   e a r t h  \n",
      "output_characters>\n",
      "[36  6 47 39 12 10 36 60]\n",
      "  c a r t h   E\n",
      "Generate Words by feeding the first character to the model at epoch-- 50\n",
      "  a n d  \n",
      "========================\n",
      "2019-11-06 23:33:48: Loss after num_examples_seen=25500 epoch=51: 0.783107\n",
      "2019-11-06 23:33:50: Loss after num_examples_seen=26000 epoch=52: 0.782756\n",
      "2019-11-06 23:33:51: Loss after num_examples_seen=26500 epoch=53: 0.782410\n",
      "2019-11-06 23:33:52: Loss after num_examples_seen=27000 epoch=54: 0.782070\n",
      "2019-11-06 23:33:53: Loss after num_examples_seen=27500 epoch=55: 0.781735\n",
      "2019-11-06 23:33:55: Loss after num_examples_seen=28000 epoch=56: 0.781406\n",
      "2019-11-06 23:33:56: Loss after num_examples_seen=28500 epoch=57: 0.781082\n",
      "2019-11-06 23:33:58: Loss after num_examples_seen=29000 epoch=58: 0.780763\n",
      "2019-11-06 23:33:59: Loss after num_examples_seen=29500 epoch=59: 0.780449\n",
      "2019-11-06 23:34:00: Loss after num_examples_seen=30000 epoch=60: 0.780139\n",
      "========================\n",
      "Start Pred -- \n",
      "input_characters>\n",
      "[5, 36, 64, 47, 39, 12, 10, 36]\n",
      "S   e a r t h  \n",
      "output_characters>\n",
      "[36  6 47 39 12 10 36 60]\n",
      "  c a r t h   E\n",
      "Generate Words by feeding the first character to the model at epoch-- 60\n",
      "  c o n l i n g e h  \n",
      "========================\n",
      "2019-11-06 23:34:02: Loss after num_examples_seen=30500 epoch=61: 0.779834\n",
      "2019-11-06 23:34:03: Loss after num_examples_seen=31000 epoch=62: 0.779532\n",
      "2019-11-06 23:34:04: Loss after num_examples_seen=31500 epoch=63: 0.779235\n",
      "2019-11-06 23:34:05: Loss after num_examples_seen=32000 epoch=64: 0.778942\n",
      "2019-11-06 23:34:07: Loss after num_examples_seen=32500 epoch=65: 0.778653\n",
      "2019-11-06 23:34:08: Loss after num_examples_seen=33000 epoch=66: 0.778367\n",
      "2019-11-06 23:34:09: Loss after num_examples_seen=33500 epoch=67: 0.778085\n",
      "2019-11-06 23:34:10: Loss after num_examples_seen=34000 epoch=68: 0.777806\n",
      "2019-11-06 23:34:11: Loss after num_examples_seen=34500 epoch=69: 0.777531\n",
      "2019-11-06 23:34:12: Loss after num_examples_seen=35000 epoch=70: 0.777259\n",
      "========================\n",
      "Start Pred -- \n",
      "input_characters>\n",
      "[5, 36, 64, 47, 39, 12, 10, 36]\n",
      "S   e a r t h  \n",
      "output_characters>\n",
      "[36  6 47 39 12 10 36 60]\n",
      "  c a r t h   E\n",
      "Generate Words by feeding the first character to the model at epoch-- 70\n",
      "  t h e  \n",
      "========================\n",
      "2019-11-06 23:34:14: Loss after num_examples_seen=35500 epoch=71: 0.776991\n",
      "2019-11-06 23:34:15: Loss after num_examples_seen=36000 epoch=72: 0.776727\n",
      "2019-11-06 23:34:16: Loss after num_examples_seen=36500 epoch=73: 0.776465\n",
      "2019-11-06 23:34:17: Loss after num_examples_seen=37000 epoch=74: 0.776207\n",
      "2019-11-06 23:34:18: Loss after num_examples_seen=37500 epoch=75: 0.775953\n",
      "2019-11-06 23:34:19: Loss after num_examples_seen=38000 epoch=76: 0.775701\n",
      "2019-11-06 23:34:20: Loss after num_examples_seen=38500 epoch=77: 0.775454\n",
      "2019-11-06 23:34:22: Loss after num_examples_seen=39000 epoch=78: 0.775209\n",
      "2019-11-06 23:34:23: Loss after num_examples_seen=39500 epoch=79: 0.774969\n",
      "2019-11-06 23:34:24: Loss after num_examples_seen=40000 epoch=80: 0.774731\n",
      "========================\n",
      "Start Pred -- \n",
      "input_characters>\n",
      "[5, 36, 64, 47, 39, 12, 10, 36]\n",
      "S   e a r t h  \n",
      "output_characters>\n",
      "[36  6 47 39 12 10 36 60]\n",
      "  c a r t h   E\n",
      "Generate Words by feeding the first character to the model at epoch-- 80\n",
      "  t h e  \n",
      "========================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-06 23:34:25: Loss after num_examples_seen=40500 epoch=81: 0.774497\n",
      "2019-11-06 23:34:26: Loss after num_examples_seen=41000 epoch=82: 0.774267\n",
      "2019-11-06 23:34:27: Loss after num_examples_seen=41500 epoch=83: 0.774040\n",
      "2019-11-06 23:34:29: Loss after num_examples_seen=42000 epoch=84: 0.773817\n",
      "2019-11-06 23:34:30: Loss after num_examples_seen=42500 epoch=85: 0.773597\n",
      "2019-11-06 23:34:31: Loss after num_examples_seen=43000 epoch=86: 0.773380\n",
      "2019-11-06 23:34:32: Loss after num_examples_seen=43500 epoch=87: 0.773168\n",
      "2019-11-06 23:34:33: Loss after num_examples_seen=44000 epoch=88: 0.772958\n",
      "2019-11-06 23:34:34: Loss after num_examples_seen=44500 epoch=89: 0.772753\n",
      "2019-11-06 23:34:36: Loss after num_examples_seen=45000 epoch=90: 0.772550\n",
      "========================\n",
      "Start Pred -- \n",
      "input_characters>\n",
      "[5, 36, 64, 47, 39, 12, 10, 36]\n",
      "S   e a r t h  \n",
      "output_characters>\n",
      "[36  6 47 39 12 10 36 60]\n",
      "  c a r t h   E\n",
      "Generate Words by feeding the first character to the model at epoch-- 90\n",
      "  c h a p t e r  \n",
      "========================\n",
      "2019-11-06 23:34:37: Loss after num_examples_seen=45500 epoch=91: 0.772351\n",
      "2019-11-06 23:34:39: Loss after num_examples_seen=46000 epoch=92: 0.772156\n",
      "2019-11-06 23:34:40: Loss after num_examples_seen=46500 epoch=93: 0.771964\n",
      "2019-11-06 23:34:42: Loss after num_examples_seen=47000 epoch=94: 0.771775\n",
      "2019-11-06 23:34:43: Loss after num_examples_seen=47500 epoch=95: 0.771590\n",
      "2019-11-06 23:34:45: Loss after num_examples_seen=48000 epoch=96: 0.771408\n",
      "2019-11-06 23:34:47: Loss after num_examples_seen=48500 epoch=97: 0.771229\n",
      "2019-11-06 23:34:48: Loss after num_examples_seen=49000 epoch=98: 0.771054\n",
      "2019-11-06 23:34:49: Loss after num_examples_seen=49500 epoch=99: 0.770881\n",
      "[0.8448749025312604, 1.1696043778959841, 0.970496665388768, 0.9008842757660296, 0.8955636795219455, 0.889125886766037, 0.8872885097423376, 0.8862288436154305, 0.8862070806511926, 0.8868966633668867, 0.8381843279409589, 0.8381802537290692, 0.8376111933005783, 0.8371596589078394, 0.8368355208170356, 0.83661464953375, 0.8364704090660103, 0.8363809469524276, 0.836329694829013, 0.8363045325879298, 0.8362967705449846, 0.8363002863251756, 0.8029951197194769, 0.8008934051442425, 0.7992086783705583, 0.7977877536040316, 0.7965705178827561, 0.7955090138001115, 0.7945697547148759, 0.7937276692653661, 0.7929634250908727, 0.7922620118519422, 0.7916117003738176, 0.791003270497286, 0.7904294440484684, 0.7898844615896526, 0.7893637588590928, 0.7888637148070005, 0.7883814533135026, 0.7879146864295676, 0.7874615902758317, 0.7870207067969432, 0.7865908660041835, 0.7861711244188526, 0.7857607162799535, 0.7853590147711963, 0.7849655010953613, 0.7845797397051011, 0.7842013584030982, 0.7838300323585091, 0.783465471353205, 0.78310740977127, 0.7827555989807211, 0.7824098018331991, 0.7820697890358101, 0.7817353371444289, 0.7814062279060289, 0.781082248655925, 0.7807631934656725, 0.7804488647466301, 0.7801390750425512, 0.7798336487890976, 0.7795324238711785, 0.7792352528638113, 0.77894200389248, 0.7786525610907483, 0.7783668246645807, 0.7780847105946432, 0.7778061500209168, 0.7775310883608727, 0.7772594842142981, 0.7769913081071403, 0.7767265411238512, 0.7764651734742156, 0.7762072030359987, 0.7759526339100791, 0.7757014750192479, 0.7754537387763522, 0.7752094398415824, 0.7749685939830934, 0.774731217049551, 0.7744973240585751, 0.7742669284007456, 0.7740400411559141, 0.773816670515948, 0.7735968213069596, 0.7733804946029693, 0.7731676874229141, 0.7729583925029747, 0.7727525981364088, 0.7725502880736348, 0.7723514414755822, 0.7721560329139294, 0.7719640324120836, 0.7717754055214564, 0.7715901134277807, 0.7714081130829842, 0.7712293573584856, 0.7710537952163148, 0.7708813718951403]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "# Train on a small subset of the data to see what happens\n",
    "model2 = RNNVanilla_2(len(chars))\n",
    "losses = model2.train_with_sgd(XTrain[:500], yTrain[:500],break_points_list=[10,20,30,40,50,60,70,80,90],\n",
    "                              nepoch=100, learning_rate=0.005, evaluate_loss_after=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gradient check after doubling hidden units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing gradient check for parameter U with size 1000.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pprusty05/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:137: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check for parameter U passed.\n",
      "Performing gradient check for parameter V with size 1000.\n",
      "Gradient check for parameter V passed.\n",
      "Performing gradient check for parameter W with size 100.\n",
      "Gradient check for parameter W passed.\n"
     ]
    }
   ],
   "source": [
    "# To avoid performing millions of expensive calculations we use a smaller vocabulary size for checking.\n",
    "grad_check_vocab_size = 100\n",
    "np.random.seed(10)\n",
    "model2 = RNNVanilla_2(grad_check_vocab_size, 10, bptt_truncate=1000)\n",
    "model2.gradient_check([0,1,2,3], [1,2,3,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Discuss your findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observation\n",
    "\n",
    "## By increasing the hidden layers\n",
    "\n",
    "-the processing time increased. \n",
    "\n",
    "-loss plot did decrease faster.\n",
    "\n",
    "-character prediction improved.\n",
    "\n",
    "## By reducing hidden layers\n",
    "\n",
    "-the processing time decresed. \n",
    "\n",
    "-loss plot did not decrease faster or as lower as with higher hidden layers.\n",
    "\n",
    "-character prediction degraded.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence length: Try doubling and halving your length of sentence that feeds into the network. And after training, plot the training loss vs the number of training epochs, and show the text sampling results. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-06 23:34:52: Loss after num_examples_seen=0 epoch=0: 4.520170\n",
      "2019-11-06 23:34:55: Loss after num_examples_seen=1000 epoch=1: 2.049460\n",
      "2019-11-06 23:34:58: Loss after num_examples_seen=2000 epoch=2: 1.902007\n",
      "2019-11-06 23:35:01: Loss after num_examples_seen=3000 epoch=3: 1.807512\n",
      "2019-11-06 23:35:03: Loss after num_examples_seen=4000 epoch=4: 1.743794\n",
      "2019-11-06 23:35:06: Loss after num_examples_seen=5000 epoch=5: 1.694558\n",
      "2019-11-06 23:35:09: Loss after num_examples_seen=6000 epoch=6: 1.653011\n",
      "2019-11-06 23:35:12: Loss after num_examples_seen=7000 epoch=7: 1.618246\n",
      "2019-11-06 23:35:15: Loss after num_examples_seen=8000 epoch=8: 1.588798\n",
      "2019-11-06 23:35:17: Loss after num_examples_seen=9000 epoch=9: 1.562903\n",
      "2019-11-06 23:35:20: Loss after num_examples_seen=10000 epoch=10: 1.539987\n",
      "========================\n",
      "Start Pred -- \n",
      "input_characters>\n",
      "[5, 36, 64, 47, 39, 12, 10, 36]\n",
      "S   e a r t h  \n",
      "output_characters>\n",
      "[36 47 39 69 36 10 64 60]\n",
      "  a r s   h e E\n",
      "Generate Words by feeding the first character to the model at epoch-- 10\n",
      "  2 o m e e f  \n",
      "========================\n",
      "2019-11-06 23:35:23: Loss after num_examples_seen=11000 epoch=11: 1.519731\n",
      "2019-11-06 23:35:28: Loss after num_examples_seen=12000 epoch=12: 1.501579\n",
      "2019-11-06 23:35:32: Loss after num_examples_seen=13000 epoch=13: 1.484134\n",
      "2019-11-06 23:35:44: Loss after num_examples_seen=14000 epoch=14: 1.466284\n",
      "2019-11-06 23:35:51: Loss after num_examples_seen=15000 epoch=15: 1.447713\n",
      "2019-11-06 23:35:55: Loss after num_examples_seen=16000 epoch=16: 1.428955\n",
      "2019-11-06 23:36:03: Loss after num_examples_seen=17000 epoch=17: 1.410977\n",
      "2019-11-06 23:36:16: Loss after num_examples_seen=18000 epoch=18: 1.394552\n",
      "2019-11-06 23:36:36: Loss after num_examples_seen=19000 epoch=19: 1.380561\n",
      "2019-11-06 23:36:50: Loss after num_examples_seen=20000 epoch=20: 1.369495\n",
      "========================\n",
      "Start Pred -- \n",
      "input_characters>\n",
      "[5, 36, 64, 47, 39, 12, 10, 36]\n",
      "S   e a r t h  \n",
      "output_characters>\n",
      "[36 47 39 63 64 10 36 60]\n",
      "  a r d e h   E\n",
      "Generate Words by feeding the first character to the model at epoch-- 20\n",
      "  r e d e d  \n",
      "========================\n",
      "2019-11-06 23:37:05: Loss after num_examples_seen=21000 epoch=21: 1.360167\n",
      "2019-11-06 23:37:18: Loss after num_examples_seen=22000 epoch=22: 1.351529\n",
      "2019-11-06 23:37:31: Loss after num_examples_seen=23000 epoch=23: 1.342808\n",
      "2019-11-06 23:37:44: Loss after num_examples_seen=24000 epoch=24: 1.335573\n",
      "2019-11-06 23:37:55: Loss after num_examples_seen=25000 epoch=25: 1.335638\n",
      "Setting learning rate to 0.002500\n",
      "2019-11-06 23:38:02: Loss after num_examples_seen=26000 epoch=26: 1.264739\n",
      "2019-11-06 23:38:08: Loss after num_examples_seen=27000 epoch=27: 1.257970\n",
      "2019-11-06 23:38:15: Loss after num_examples_seen=28000 epoch=28: 1.253606\n",
      "2019-11-06 23:38:21: Loss after num_examples_seen=29000 epoch=29: 1.249845\n",
      "2019-11-06 23:38:27: Loss after num_examples_seen=30000 epoch=30: 1.246920\n",
      "========================\n",
      "Start Pred -- \n",
      "input_characters>\n",
      "[5, 36, 64, 47, 39, 12, 10, 36]\n",
      "S   e a r t h  \n",
      "output_characters>\n",
      "[36 47 83 92 12 10 36 60]\n",
      "  a n g t h   E\n",
      "Generate Words by feeding the first character to the model at epoch-- 30\n",
      "  w a s  \n",
      "========================\n",
      "2019-11-06 23:38:33: Loss after num_examples_seen=31000 epoch=31: 1.244653\n",
      "2019-11-06 23:38:39: Loss after num_examples_seen=32000 epoch=32: 1.243000\n",
      "2019-11-06 23:38:45: Loss after num_examples_seen=33000 epoch=33: 1.241714\n",
      "2019-11-06 23:38:51: Loss after num_examples_seen=34000 epoch=34: 1.240547\n",
      "2019-11-06 23:38:58: Loss after num_examples_seen=35000 epoch=35: 1.239813\n",
      "2019-11-06 23:39:04: Loss after num_examples_seen=36000 epoch=36: 1.239470\n",
      "2019-11-06 23:39:10: Loss after num_examples_seen=37000 epoch=37: 1.239585\n",
      "Setting learning rate to 0.001250\n",
      "2019-11-06 23:39:16: Loss after num_examples_seen=38000 epoch=38: 1.194160\n",
      "2019-11-06 23:39:22: Loss after num_examples_seen=39000 epoch=39: 1.190719\n",
      "2019-11-06 23:39:28: Loss after num_examples_seen=40000 epoch=40: 1.188158\n",
      "========================\n",
      "Start Pred -- \n",
      "input_characters>\n",
      "[5, 36, 64, 47, 39, 12, 10, 36]\n",
      "S   e a r t h  \n",
      "output_characters>\n",
      "[36 47 83 39 12 10 36 60]\n",
      "  a n r t h   E\n",
      "Generate Words by feeding the first character to the model at epoch-- 40\n",
      "  u n c l e  \n",
      "========================\n",
      "2019-11-06 23:39:34: Loss after num_examples_seen=41000 epoch=41: 1.185920\n",
      "2019-11-06 23:39:40: Loss after num_examples_seen=42000 epoch=42: 1.183841\n",
      "2019-11-06 23:39:45: Loss after num_examples_seen=43000 epoch=43: 1.181922\n",
      "2019-11-06 23:39:48: Loss after num_examples_seen=44000 epoch=44: 1.180150\n",
      "2019-11-06 23:39:51: Loss after num_examples_seen=45000 epoch=45: 1.178478\n",
      "2019-11-06 23:39:54: Loss after num_examples_seen=46000 epoch=46: 1.176912\n",
      "2019-11-06 23:39:57: Loss after num_examples_seen=47000 epoch=47: 1.175395\n",
      "2019-11-06 23:40:01: Loss after num_examples_seen=48000 epoch=48: 1.173967\n",
      "2019-11-06 23:40:04: Loss after num_examples_seen=49000 epoch=49: 1.172569\n",
      "2019-11-06 23:40:07: Loss after num_examples_seen=50000 epoch=50: 1.171265\n",
      "========================\n",
      "Start Pred -- \n",
      "input_characters>\n",
      "[5, 36, 64, 47, 39, 12, 10, 36]\n",
      "S   e a r t h  \n",
      "output_characters>\n",
      "[36 47 83 39 12 10 36 60]\n",
      "  a n r t h   E\n",
      "Generate Words by feeding the first character to the model at epoch-- 50\n",
      "  p r o f e s t o r  \n",
      "========================\n",
      "2019-11-06 23:40:11: Loss after num_examples_seen=51000 epoch=51: 1.169989\n",
      "2019-11-06 23:40:14: Loss after num_examples_seen=52000 epoch=52: 1.168823\n",
      "2019-11-06 23:40:17: Loss after num_examples_seen=53000 epoch=53: 1.167686\n",
      "2019-11-06 23:40:20: Loss after num_examples_seen=54000 epoch=54: 1.166676\n",
      "2019-11-06 23:40:23: Loss after num_examples_seen=55000 epoch=55: 1.165698\n",
      "2019-11-06 23:40:26: Loss after num_examples_seen=56000 epoch=56: 1.164852\n",
      "2019-11-06 23:40:30: Loss after num_examples_seen=57000 epoch=57: 1.164050\n",
      "2019-11-06 23:40:33: Loss after num_examples_seen=58000 epoch=58: 1.163370\n",
      "2019-11-06 23:40:36: Loss after num_examples_seen=59000 epoch=59: 1.162753\n",
      "2019-11-06 23:40:39: Loss after num_examples_seen=60000 epoch=60: 1.162230\n",
      "========================\n",
      "Start Pred -- \n",
      "input_characters>\n",
      "[5, 36, 64, 47, 39, 12, 10, 36]\n",
      "S   e a r t h  \n",
      "output_characters>\n",
      "[36 47 83 39 12 10 36 60]\n",
      "  a n r t h   E\n",
      "Generate Words by feeding the first character to the model at epoch-- 60\n",
      "  a d o c h  \n",
      "========================\n",
      "2019-11-06 23:40:42: Loss after num_examples_seen=61000 epoch=61: 1.161800\n",
      "2019-11-06 23:40:45: Loss after num_examples_seen=62000 epoch=62: 1.161433\n",
      "2019-11-06 23:40:49: Loss after num_examples_seen=63000 epoch=63: 1.161090\n",
      "2019-11-06 23:40:52: Loss after num_examples_seen=64000 epoch=64: 1.160996\n",
      "2019-11-06 23:40:55: Loss after num_examples_seen=65000 epoch=65: 1.160610\n",
      "2019-11-06 23:40:58: Loss after num_examples_seen=66000 epoch=66: 1.160826\n",
      "Setting learning rate to 0.000625\n",
      "2019-11-06 23:41:02: Loss after num_examples_seen=67000 epoch=67: 1.130664\n",
      "2019-11-06 23:41:05: Loss after num_examples_seen=68000 epoch=68: 1.121665\n",
      "2019-11-06 23:41:08: Loss after num_examples_seen=69000 epoch=69: 1.118005\n",
      "2019-11-06 23:41:11: Loss after num_examples_seen=70000 epoch=70: 1.115278\n",
      "========================\n",
      "Start Pred -- \n",
      "input_characters>\n",
      "[5, 36, 64, 47, 39, 12, 10, 36]\n",
      "S   e a r t h  \n",
      "output_characters>\n",
      "[36 12 83 39 12 10 36 60]\n",
      "  t n r t h   E\n",
      "Generate Words by feeding the first character to the model at epoch-- 70\n",
      "  c e n o r i s  \n",
      "========================\n",
      "2019-11-06 23:41:14: Loss after num_examples_seen=71000 epoch=71: 1.113232\n",
      "2019-11-06 23:41:18: Loss after num_examples_seen=72000 epoch=72: 1.111462\n",
      "2019-11-06 23:41:21: Loss after num_examples_seen=73000 epoch=73: 1.109952\n",
      "2019-11-06 23:41:24: Loss after num_examples_seen=74000 epoch=74: 1.108583\n",
      "2019-11-06 23:41:27: Loss after num_examples_seen=75000 epoch=75: 1.107343\n",
      "2019-11-06 23:41:30: Loss after num_examples_seen=76000 epoch=76: 1.106188\n",
      "2019-11-06 23:41:33: Loss after num_examples_seen=77000 epoch=77: 1.105111\n",
      "2019-11-06 23:41:37: Loss after num_examples_seen=78000 epoch=78: 1.104091\n",
      "2019-11-06 23:41:40: Loss after num_examples_seen=79000 epoch=79: 1.103125\n",
      "2019-11-06 23:41:43: Loss after num_examples_seen=80000 epoch=80: 1.102202\n",
      "========================\n",
      "Start Pred -- \n",
      "input_characters>\n",
      "[5, 36, 64, 47, 39, 12, 10, 36]\n",
      "S   e a r t h  \n",
      "output_characters>\n",
      "[36 12 83 39 12 10 36 60]\n",
      "  t n r t h   E\n",
      "Generate Words by feeding the first character to the model at epoch-- 80\n",
      "  p r o j e c t  \n",
      "========================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-06 23:41:46: Loss after num_examples_seen=81000 epoch=81: 1.101320\n",
      "2019-11-06 23:41:49: Loss after num_examples_seen=82000 epoch=82: 1.100473\n",
      "2019-11-06 23:41:53: Loss after num_examples_seen=83000 epoch=83: 1.099660\n",
      "2019-11-06 23:41:56: Loss after num_examples_seen=84000 epoch=84: 1.098877\n",
      "2019-11-06 23:41:59: Loss after num_examples_seen=85000 epoch=85: 1.098123\n",
      "2019-11-06 23:42:02: Loss after num_examples_seen=86000 epoch=86: 1.097396\n",
      "2019-11-06 23:42:05: Loss after num_examples_seen=87000 epoch=87: 1.096696\n",
      "2019-11-06 23:42:09: Loss after num_examples_seen=88000 epoch=88: 1.096019\n",
      "2019-11-06 23:42:12: Loss after num_examples_seen=89000 epoch=89: 1.095367\n",
      "2019-11-06 23:42:15: Loss after num_examples_seen=90000 epoch=90: 1.094737\n",
      "========================\n",
      "Start Pred -- \n",
      "input_characters>\n",
      "[5, 36, 64, 47, 39, 12, 10, 36]\n",
      "S   e a r t h  \n",
      "output_characters>\n",
      "[36 12 83 39 12 10 36 60]\n",
      "  t n r t h   E\n",
      "Generate Words by feeding the first character to the model at epoch-- 90\n",
      "  t e r r s o n  \n",
      "========================\n",
      "2019-11-06 23:42:18: Loss after num_examples_seen=91000 epoch=91: 1.094130\n",
      "2019-11-06 23:42:21: Loss after num_examples_seen=92000 epoch=92: 1.093544\n",
      "2019-11-06 23:42:25: Loss after num_examples_seen=93000 epoch=93: 1.092980\n",
      "2019-11-06 23:42:28: Loss after num_examples_seen=94000 epoch=94: 1.092435\n",
      "2019-11-06 23:42:31: Loss after num_examples_seen=95000 epoch=95: 1.091911\n",
      "2019-11-06 23:42:34: Loss after num_examples_seen=96000 epoch=96: 1.091405\n",
      "2019-11-06 23:42:37: Loss after num_examples_seen=97000 epoch=97: 1.090921\n",
      "2019-11-06 23:42:40: Loss after num_examples_seen=98000 epoch=98: 1.090454\n",
      "2019-11-06 23:42:44: Loss after num_examples_seen=99000 epoch=99: 1.090009\n",
      "[4.520170051748453, 2.04945993206512, 1.9020066994896687, 1.8075119726433209, 1.7437937409040096, 1.6945576521731556, 1.6530113441089553, 1.6182456569941814, 1.5887984979216299, 1.5629026620630586, 1.5399874942418916, 1.5197312810487356, 1.501578816803639, 1.4841336487793884, 1.4662835568122312, 1.4477130078401332, 1.428955440267003, 1.4109769102429226, 1.3945518072613508, 1.3805614502540033, 1.3694953718005276, 1.3601674638914225, 1.35152861347461, 1.3428079592812583, 1.3355733777958203, 1.335637667261413, 1.264739362335492, 1.2579700018725846, 1.2536061785775803, 1.2498451352620288, 1.2469204508825462, 1.244652642291014, 1.242999567793543, 1.2417143209473744, 1.240546912113138, 1.239812741000338, 1.2394698425553192, 1.2395852213420175, 1.1941604892023194, 1.190718571822028, 1.18815845821091, 1.185920347279086, 1.1838414560828916, 1.1819222815885735, 1.180150325893567, 1.1784781414065784, 1.176911551744401, 1.175395324945577, 1.1739669283966763, 1.172568960773705, 1.17126549072899, 1.169988508336988, 1.1688233753411552, 1.167685889813551, 1.1666758390461354, 1.1656978220090515, 1.1648524718993358, 1.1640495417059187, 1.1633696873264614, 1.1627528614772198, 1.1622296742923428, 1.1617999376709371, 1.1614332969399193, 1.161090314601762, 1.1609964503031567, 1.1606095596100525, 1.1608261458707856, 1.130664129170272, 1.12166473229727, 1.1180053788347195, 1.1152779860094486, 1.1132324283174149, 1.1114617371137847, 1.1099515081140316, 1.1085829457090144, 1.1073430002324898, 1.106188263909633, 1.1051106066006124, 1.1040909228864542, 1.1031245535734104, 1.1022016748262844, 1.101319674824892, 1.1004728495949916, 1.0996597977020615, 1.0988768048396558, 1.0981231911343228, 1.097396215496867, 1.0966956789822246, 1.0960192899282677, 1.0953672577246867, 1.0947374333035593, 1.0941304742449567, 1.0935441486568962, 1.0929797037929603, 1.0924345862999725, 1.0919108789118415, 1.0914053208696977, 1.0909212210353751, 1.0904538846093441, 1.0900085825100039]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#increasing length of sentence that feeds into the network\n",
    "np.random.seed(10)\n",
    "# Train on a small subset of the data to see what happens\n",
    "model = RNNVanilla(len(chars))\n",
    "losses = model.train_with_sgd(XTrain[:1000], yTrain[:1000],break_points_list=[10,20,30,40,50,60,70,80,90],\n",
    "                              nepoch=100, learning_rate=0.005, evaluate_loss_after=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-06 23:42:47: Loss after num_examples_seen=0 epoch=0: 4.521373\n",
      "2019-11-06 23:42:47: Loss after num_examples_seen=100 epoch=1: 4.342255\n",
      "2019-11-06 23:42:47: Loss after num_examples_seen=200 epoch=2: 3.061407\n",
      "2019-11-06 23:42:48: Loss after num_examples_seen=300 epoch=3: 2.545218\n",
      "2019-11-06 23:42:48: Loss after num_examples_seen=400 epoch=4: 2.340414\n",
      "2019-11-06 23:42:48: Loss after num_examples_seen=500 epoch=5: 2.232282\n",
      "2019-11-06 23:42:49: Loss after num_examples_seen=600 epoch=6: 2.156400\n",
      "2019-11-06 23:42:49: Loss after num_examples_seen=700 epoch=7: 2.099870\n",
      "2019-11-06 23:42:49: Loss after num_examples_seen=800 epoch=8: 2.055378\n",
      "2019-11-06 23:42:50: Loss after num_examples_seen=900 epoch=9: 2.018892\n",
      "2019-11-06 23:42:50: Loss after num_examples_seen=1000 epoch=10: 1.987732\n",
      "========================\n",
      "Start Pred -- \n",
      "input_characters>\n",
      "[5, 36, 64, 47, 39, 12, 10, 36]\n",
      "S   e a r t h  \n",
      "output_characters>\n",
      "[36 36 64 36 64 36 36 60]\n",
      "    e   e     E\n",
      "Generate Words by feeding the first character to the model at epoch-- 10\n",
      "  j e h t t s l e e c o\n",
      "========================\n",
      "2019-11-06 23:42:50: Loss after num_examples_seen=1100 epoch=11: 1.959894\n",
      "2019-11-06 23:42:51: Loss after num_examples_seen=1200 epoch=12: 1.934209\n",
      "2019-11-06 23:42:51: Loss after num_examples_seen=1300 epoch=13: 1.910145\n",
      "2019-11-06 23:42:51: Loss after num_examples_seen=1400 epoch=14: 1.887359\n",
      "2019-11-06 23:42:52: Loss after num_examples_seen=1500 epoch=15: 1.865538\n",
      "2019-11-06 23:42:52: Loss after num_examples_seen=1600 epoch=16: 1.844450\n",
      "2019-11-06 23:42:52: Loss after num_examples_seen=1700 epoch=17: 1.823933\n",
      "2019-11-06 23:42:52: Loss after num_examples_seen=1800 epoch=18: 1.803849\n",
      "2019-11-06 23:42:53: Loss after num_examples_seen=1900 epoch=19: 1.784064\n",
      "2019-11-06 23:42:53: Loss after num_examples_seen=2000 epoch=20: 1.764422\n",
      "========================\n",
      "Start Pred -- \n",
      "input_characters>\n",
      "[5, 36, 64, 47, 39, 12, 10, 36]\n",
      "S   e a r t h  \n",
      "output_characters>\n",
      "[36 36 83 12 64 36 64 60]\n",
      "    n t e   e E\n",
      "Generate Words by feeding the first character to the model at epoch-- 20\n",
      "  d M n e  \n",
      "========================\n",
      "2019-11-06 23:42:53: Loss after num_examples_seen=2100 epoch=21: 1.744759\n",
      "2019-11-06 23:42:54: Loss after num_examples_seen=2200 epoch=22: 1.724940\n",
      "2019-11-06 23:42:54: Loss after num_examples_seen=2300 epoch=23: 1.704925\n",
      "2019-11-06 23:42:54: Loss after num_examples_seen=2400 epoch=24: 1.684790\n",
      "2019-11-06 23:42:55: Loss after num_examples_seen=2500 epoch=25: 1.664707\n",
      "2019-11-06 23:42:55: Loss after num_examples_seen=2600 epoch=26: 1.644897\n",
      "2019-11-06 23:42:55: Loss after num_examples_seen=2700 epoch=27: 1.625583\n",
      "2019-11-06 23:42:56: Loss after num_examples_seen=2800 epoch=28: 1.606940\n",
      "2019-11-06 23:42:56: Loss after num_examples_seen=2900 epoch=29: 1.589056\n",
      "2019-11-06 23:42:56: Loss after num_examples_seen=3000 epoch=30: 1.571899\n",
      "========================\n",
      "Start Pred -- \n",
      "input_characters>\n",
      "[5, 36, 64, 47, 39, 12, 10, 36]\n",
      "S   e a r t h  \n",
      "output_characters>\n",
      "[36 36 83 12 64 10 64 60]\n",
      "    n t e h e E\n",
      "Generate Words by feeding the first character to the model at epoch-- 30\n",
      "  w n t l s b y o w g 7 a t  \n",
      "========================\n",
      "2019-11-06 23:42:57: Loss after num_examples_seen=3100 epoch=31: 1.555292\n",
      "2019-11-06 23:42:57: Loss after num_examples_seen=3200 epoch=32: 1.538948\n",
      "2019-11-06 23:42:57: Loss after num_examples_seen=3300 epoch=33: 1.522542\n",
      "2019-11-06 23:42:58: Loss after num_examples_seen=3400 epoch=34: 1.505780\n",
      "2019-11-06 23:42:58: Loss after num_examples_seen=3500 epoch=35: 1.488400\n",
      "2019-11-06 23:42:58: Loss after num_examples_seen=3600 epoch=36: 1.470184\n",
      "2019-11-06 23:42:59: Loss after num_examples_seen=3700 epoch=37: 1.451013\n",
      "2019-11-06 23:42:59: Loss after num_examples_seen=3800 epoch=38: 1.430948\n",
      "2019-11-06 23:42:59: Loss after num_examples_seen=3900 epoch=39: 1.410287\n",
      "2019-11-06 23:43:00: Loss after num_examples_seen=4000 epoch=40: 1.389531\n",
      "========================\n",
      "Start Pred -- \n",
      "input_characters>\n",
      "[5, 36, 64, 47, 39, 12, 10, 36]\n",
      "S   e a r t h  \n",
      "output_characters>\n",
      "[36 36 83 12 12 10 36 60]\n",
      "    n t t h   E\n",
      "Generate Words by feeding the first character to the model at epoch-- 40\n",
      "  g b l ) l c 9\n",
      "========================\n",
      "2019-11-06 23:43:00: Loss after num_examples_seen=4100 epoch=41: 1.369197\n",
      "2019-11-06 23:43:00: Loss after num_examples_seen=4200 epoch=42: 1.349563\n",
      "2019-11-06 23:43:01: Loss after num_examples_seen=4300 epoch=43: 1.330661\n",
      "2019-11-06 23:43:01: Loss after num_examples_seen=4400 epoch=44: 1.312459\n",
      "2019-11-06 23:43:01: Loss after num_examples_seen=4500 epoch=45: 1.294953\n",
      "2019-11-06 23:43:02: Loss after num_examples_seen=4600 epoch=46: 1.278092\n",
      "2019-11-06 23:43:02: Loss after num_examples_seen=4700 epoch=47: 1.261727\n",
      "2019-11-06 23:43:02: Loss after num_examples_seen=4800 epoch=48: 1.245681\n",
      "2019-11-06 23:43:03: Loss after num_examples_seen=4900 epoch=49: 1.229836\n",
      "2019-11-06 23:43:03: Loss after num_examples_seen=5000 epoch=50: 1.214175\n",
      "========================\n",
      "Start Pred -- \n",
      "input_characters>\n",
      "[5, 36, 64, 47, 39, 12, 10, 36]\n",
      "S   e a r t h  \n",
      "output_characters>\n",
      "[36 36 83 12 12 10 36 60]\n",
      "    n t t h   E\n",
      "Generate Words by feeding the first character to the model at epoch-- 50\n",
      "  2 h e r a c t e  \n",
      "========================\n",
      "2019-11-06 23:43:03: Loss after num_examples_seen=5100 epoch=51: 1.198780\n",
      "2019-11-06 23:43:04: Loss after num_examples_seen=5200 epoch=52: 1.183823\n",
      "2019-11-06 23:43:04: Loss after num_examples_seen=5300 epoch=53: 1.169553\n",
      "2019-11-06 23:43:04: Loss after num_examples_seen=5400 epoch=54: 1.156307\n",
      "2019-11-06 23:43:05: Loss after num_examples_seen=5500 epoch=55: 1.144546\n",
      "2019-11-06 23:43:05: Loss after num_examples_seen=5600 epoch=56: 1.134773\n",
      "2019-11-06 23:43:05: Loss after num_examples_seen=5700 epoch=57: 1.127217\n",
      "2019-11-06 23:43:06: Loss after num_examples_seen=5800 epoch=58: 1.121530\n",
      "2019-11-06 23:43:06: Loss after num_examples_seen=5900 epoch=59: 1.116900\n",
      "2019-11-06 23:43:06: Loss after num_examples_seen=6000 epoch=60: 1.112468\n",
      "========================\n",
      "Start Pred -- \n",
      "input_characters>\n",
      "[5, 36, 64, 47, 39, 12, 10, 36]\n",
      "S   e a r t h  \n",
      "output_characters>\n",
      "[36 36 83 12 12 10 57 60]\n",
      "    n t t h i E\n",
      "Generate Words by feeding the first character to the model at epoch-- 60\n",
      "  a n y 2 i c %  \n",
      "========================\n",
      "2019-11-06 23:43:07: Loss after num_examples_seen=6100 epoch=61: 1.107609\n",
      "2019-11-06 23:43:07: Loss after num_examples_seen=6200 epoch=62: 1.101981\n",
      "2019-11-06 23:43:07: Loss after num_examples_seen=6300 epoch=63: 1.095461\n",
      "2019-11-06 23:43:08: Loss after num_examples_seen=6400 epoch=64: 1.088068\n",
      "2019-11-06 23:43:08: Loss after num_examples_seen=6500 epoch=65: 1.079911\n",
      "2019-11-06 23:43:08: Loss after num_examples_seen=6600 epoch=66: 1.071140\n",
      "2019-11-06 23:43:09: Loss after num_examples_seen=6700 epoch=67: 1.061930\n",
      "2019-11-06 23:43:09: Loss after num_examples_seen=6800 epoch=68: 1.052461\n",
      "2019-11-06 23:43:09: Loss after num_examples_seen=6900 epoch=69: 1.042902\n",
      "2019-11-06 23:43:09: Loss after num_examples_seen=7000 epoch=70: 1.033386\n",
      "========================\n",
      "Start Pred -- \n",
      "input_characters>\n",
      "[5, 36, 64, 47, 39, 12, 10, 36]\n",
      "S   e a r t h  \n",
      "output_characters>\n",
      "[36 36 83  6 12 10 57 60]\n",
      "    n c t h i E\n",
      "Generate Words by feeding the first character to the model at epoch-- 70\n",
      "  k a c g u s g  \n",
      "========================\n",
      "2019-11-06 23:43:10: Loss after num_examples_seen=7100 epoch=71: 1.024007\n",
      "2019-11-06 23:43:10: Loss after num_examples_seen=7200 epoch=72: 1.014819\n",
      "2019-11-06 23:43:10: Loss after num_examples_seen=7300 epoch=73: 1.005850\n",
      "2019-11-06 23:43:11: Loss after num_examples_seen=7400 epoch=74: 0.997107\n",
      "2019-11-06 23:43:11: Loss after num_examples_seen=7500 epoch=75: 0.988591\n",
      "2019-11-06 23:43:11: Loss after num_examples_seen=7600 epoch=76: 0.980299\n",
      "2019-11-06 23:43:12: Loss after num_examples_seen=7700 epoch=77: 0.972228\n",
      "2019-11-06 23:43:12: Loss after num_examples_seen=7800 epoch=78: 0.964375\n",
      "2019-11-06 23:43:12: Loss after num_examples_seen=7900 epoch=79: 0.956743\n",
      "2019-11-06 23:43:13: Loss after num_examples_seen=8000 epoch=80: 0.949332\n",
      "========================\n",
      "Start Pred -- \n",
      "input_characters>\n",
      "[5, 36, 64, 47, 39, 12, 10, 36]\n",
      "S   e a r t h  \n",
      "output_characters>\n",
      "[36 36 83 39 12 10 36 60]\n",
      "    n r t h   E\n",
      "Generate Words by feeding the first character to the model at epoch-- 80\n",
      "  p r c l n c h e  \n",
      "========================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-06 23:43:13: Loss after num_examples_seen=8100 epoch=81: 0.942148\n",
      "2019-11-06 23:43:13: Loss after num_examples_seen=8200 epoch=82: 0.935194\n",
      "2019-11-06 23:43:14: Loss after num_examples_seen=8300 epoch=83: 0.928475\n",
      "2019-11-06 23:43:14: Loss after num_examples_seen=8400 epoch=84: 0.921993\n",
      "2019-11-06 23:43:14: Loss after num_examples_seen=8500 epoch=85: 0.915752\n",
      "2019-11-06 23:43:15: Loss after num_examples_seen=8600 epoch=86: 0.909752\n",
      "2019-11-06 23:43:15: Loss after num_examples_seen=8700 epoch=87: 0.903993\n",
      "2019-11-06 23:43:15: Loss after num_examples_seen=8800 epoch=88: 0.898472\n",
      "2019-11-06 23:43:16: Loss after num_examples_seen=8900 epoch=89: 0.893184\n",
      "2019-11-06 23:43:16: Loss after num_examples_seen=9000 epoch=90: 0.888125\n",
      "========================\n",
      "Start Pred -- \n",
      "input_characters>\n",
      "[5, 36, 64, 47, 39, 12, 10, 36]\n",
      "S   e a r t h  \n",
      "output_characters>\n",
      "[36 36 83 39 12 10 36 60]\n",
      "    n r t h   E\n",
      "Generate Words by feeding the first character to the model at epoch-- 90\n",
      "  l a c e n a y  \n",
      "========================\n",
      "2019-11-06 23:43:16: Loss after num_examples_seen=9100 epoch=91: 0.883287\n",
      "2019-11-06 23:43:17: Loss after num_examples_seen=9200 epoch=92: 0.878660\n",
      "2019-11-06 23:43:17: Loss after num_examples_seen=9300 epoch=93: 0.874233\n",
      "2019-11-06 23:43:17: Loss after num_examples_seen=9400 epoch=94: 0.869997\n",
      "2019-11-06 23:43:17: Loss after num_examples_seen=9500 epoch=95: 0.865936\n",
      "2019-11-06 23:43:18: Loss after num_examples_seen=9600 epoch=96: 0.862039\n",
      "2019-11-06 23:43:18: Loss after num_examples_seen=9700 epoch=97: 0.858293\n",
      "2019-11-06 23:43:18: Loss after num_examples_seen=9800 epoch=98: 0.854682\n",
      "2019-11-06 23:43:19: Loss after num_examples_seen=9900 epoch=99: 0.851196\n",
      "[4.521372995709041, 4.3422547908295295, 3.061407419406783, 2.545218345574165, 2.340414228441605, 2.232282053630649, 2.1564001718734174, 2.099870186860601, 2.055378068505672, 2.018891821262946, 1.9877317267318444, 1.9598944867287047, 1.934208810320062, 1.9101452658223963, 1.8873594255790784, 1.865537554335903, 1.8444496402720885, 1.8239326329685885, 1.8038493508109008, 1.7840638432207296, 1.7644223357568205, 1.7447585083495605, 1.7249396425493724, 1.7049249093002763, 1.6847899737145435, 1.664706505728692, 1.644896914988243, 1.6255831841563158, 1.6069397855791574, 1.5890555761721878, 1.5718985115828488, 1.5552924443475054, 1.5389475283275065, 1.5225416453852485, 1.5057796142198066, 1.4883999347032044, 1.470184030253832, 1.451012899781419, 1.4309481321158855, 1.410286954337217, 1.3895314076796288, 1.3691969511411703, 1.349563195755779, 1.3306610323187829, 1.3124594062178045, 1.2949532603570992, 1.2780919415285412, 1.2617269892180356, 1.245680948334014, 1.229836470188594, 1.2141749572674727, 1.1987800395035908, 1.1838232853637227, 1.1695528061279301, 1.1563073822690877, 1.1445461834281752, 1.1347726941537264, 1.1272174552635505, 1.1215302514089986, 1.1169002603590734, 1.112468206899538, 1.1076093551895696, 1.1019813089933999, 1.0954606085299246, 1.0880681748142997, 1.0799105861858822, 1.0711396748167339, 1.061929699917129, 1.0524612451622815, 1.0429016544737748, 1.0333857739653918, 1.0240067804524586, 1.0148193782803778, 1.005849906462844, 0.9971072378220737, 0.9885913765079903, 0.9802991630996994, 0.9722275513195996, 0.964375142378816, 0.9567425934469354, 0.9493323730078791, 0.9421481907140729, 0.9351943133890571, 0.9284748985549095, 0.921993422483956, 0.9157522394040982, 0.9097522768516765, 0.9039928498378969, 0.8984715654261205, 0.8931842897851883, 0.8881251594752789, 0.883286632880058, 0.8786595899774577, 0.8742334931221207, 0.8699966156038696, 0.8659363311059928, 0.8620394425568059, 0.8582925200678769, 0.8546822177267124, 0.8511955463826426]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#decreasing length of sentence that feeds into the network\n",
    "np.random.seed(10)\n",
    "# Train on a small subset of the data to see what happens\n",
    "model = RNNVanilla(len(chars))\n",
    "losses = model.train_with_sgd(XTrain[:100], yTrain[:100],break_points_list=[10,20,30,40,50,60,70,80,90],\n",
    "                              nepoch=100, learning_rate=0.005, evaluate_loss_after=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discuss your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observation\n",
    "\n",
    "## By increasing the training samples fed to the network\n",
    "\n",
    "-the processing time increased. \n",
    "\n",
    "-character prediction improved.\n",
    "\n",
    "-loss curve remain same.\n",
    "\n",
    "## By reducing hidden layers\n",
    "\n",
    "-the processing time decresed. \n",
    "\n",
    "-character prediction degraded. \n",
    "\n",
    "-loss curve decrease faster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
